{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Notebook: Data as geometry\n",
    "*Introduction to intelligent systems - the toolbox* \n",
    "\n",
    "** Objectives **: The objectives of this notebook is to understand (i) ..., (ii) ... and (iii) ...\n",
    "\n",
    "***\n",
    "\n",
    "## Introduction\n",
    "We will in this notebook investigate how the following simple neural network works:\n",
    "\n",
    "<img src=\"src/figures/network.PNG\",width=500,height=500>\n",
    "\n",
    "## Data\n",
    "We will adjust our network such that it can distingues between two classes of 2D points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lets generate 500 points from two classes\n",
    "N = 500\n",
    "t = np.linspace(0, 360, N)\n",
    "x1 = np.random.normal(size=N, scale=1.3)\n",
    "y1 = np.random.normal(size=N, scale=1.3)\n",
    "x2 = 6*np.cos(t) + np.random.normal(size=N, scale=0.5)\n",
    "y2 = 6*np.sin(t) + np.random.normal(size=N, scale=0.5)\n",
    "X = np.vstack((np.array([x1,y1]).T, np.array([x2, y2]).T))\n",
    "Y = np.concatenate([np.ones((N,)), np.zeros((N,))])\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.gca()\n",
    "ax.plot(x1, y1, 'b.', label='class 1')\n",
    "ax.plot(x2, y2, 'r.', label='class 2')\n",
    "ax.legend()\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "* Can you explain how the 2D points were generated?\n",
    "* Adjust the weights such that ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change w1, w2, w3\n",
    "w1 = np.array([0.64, 0.41])\n",
    "w2 = np.array([0.053, -0.71])\n",
    "w3 = np.array([-0.67, 0.33])\n",
    "\n",
    "# Plot the decision boundaries\n",
    "bias = -1.5\n",
    "xx=np.linspace(-8, 8, 10)\n",
    "l1_0=-(w1[0]*xx+bias)/w1[1]\n",
    "l2_0=-(w2[0]*xx+bias)/w2[1]\n",
    "l3_0=-(w3[0]*xx+bias)/w3[1]\n",
    "fig = plt.figure(figsize=(5, 5)); ax=fig.gca()\n",
    "ax.plot(xx, l1_0, label='neuron 1')\n",
    "ax.plot(xx, l2_0, label='neuron 2')\n",
    "ax.plot(xx, l3_0, label='neuron 3')\n",
    "ax.plot(x1, y1, 'b.', label='class 1')\n",
    "ax.plot(x2, y2, 'r.', label='class 2')\n",
    "ax.legend(bbox_to_anchor=(1, 1))\n",
    "ax.axis([-8, 8, -8, 8])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x, y = np.meshgrid(np.linspace(-8, 8, n), np.linspace(-8, 8, n))\n",
    "p = np.array([x.flatten(), y.flatten()])\n",
    "\n",
    "l1 = np.tanh(w1.T.dot(p) + bias).reshape(n, n)\n",
    "l2 = np.tanh(w2.T.dot(p) + bias).reshape(n, n)\n",
    "l3 = np.tanh(w3.T.dot(p) + bias).reshape(n, n)\n",
    "l = np.array([l1, l2, l3])\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(15, 5))\n",
    "for i in range(3):\n",
    "    ax[i].plot(x1, y1, 'b.', label='class 1')\n",
    "    ax[i].plot(x2, y2, 'r.', label='class 2')\n",
    "    cf = ax[i].contourf(x, y, l[i], 100)\n",
    "fig.colorbar(cf, ax=ax[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([w1, w2, w3])\n",
    "H = np.tanh(W.dot(X.T)+bias)\n",
    "x, y = np.meshgrid(np.linspace(-1, 1, n), np.linspace(-1, 1, n))\n",
    "z = np.tanh(-(1.6*x + 1.6*y) / 1.6)\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(H[0,:N], H[1,:N], H[2,:N], c='b', label='class 1')\n",
    "ax.scatter(H[0,N:], H[1,N:], H[2,N:], c='r', label='class 2')\n",
    "ax.quiver([0],[0],[0],[-1.6],[-1.6],[-1.6])\n",
    "ax.plot_surface(x,y,z,alpha=0.2)\n",
    "ax.set_xlabel('h1-neuron 1')\n",
    "ax.set_ylabel('h2-neuron 2')\n",
    "ax.set_zlabel('h3-neuron 3')\n",
    "ax.axis('equal')\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust w such that most of the points in class 1 gets assigned a positive value,\n",
    "# while most points in class 2 gets assignmed a negative value\n",
    "w = np.array([-1.6, -1.6, -1.6])\n",
    "\n",
    "# Compute output activation\n",
    "bias = -2.2\n",
    "hid = np.array([l1.flatten(), l2.flatten(), l3.flatten()])\n",
    "activation = np.tanh(w.T.dot(hid).reshape(n, n) + bias)\n",
    "\n",
    "# Plot output activation\n",
    "fig = plt.figure(figsize=(5, 5)); ax=fig.gca();\n",
    "ax.plot(x1, y1, 'b.', label='class 1')\n",
    "ax.plot(x2, y2, 'r.', label='class 2')\n",
    "cf=ax.contourf(x, y, activation, 100)\n",
    "ax.legend(bbox_to_anchor=(1.6, 0.5))\n",
    "fig.colorbar(cf,ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: tensorflow playground\n",
    "The neural network percented here is very simple, but the principals hold for more complicated network. Try opening [tensorflow playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.30449&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) where we can play around with number of neurons in our network, activation function ect. Try out the following:\n",
    "- Can you recreate the network we have just investigated?\n",
    "- What happens with the decision boundary when we change activation function from $tanh(\\cdot)$ to $relu(\\cdot)$?\n",
    "- Try changing the dataset to the spiral. Can you find a network that can classify this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
