{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will show some interesting AI tools used on text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"../global_setup.py\") as setupfile:\n",
    "        exec(setupfile.read())\n",
    "except FileNotFoundError:\n",
    "    print('Setup already completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from notebooks.exercises.src.text.bag_of_words import BagOfWords\n",
    "from notebooks.exercises.src.text.sentiment_viewing import ViewSentiment\n",
    "from notebooks.exercises.src.text.wiki_search_dash import WikipediaSearchDashboard\n",
    "from notebooks.exercises.src.text.twittipedia import Twittipedia\n",
    "from src.text.document_retrieval.wikipedia import Wikipedia\n",
    "from src.text.twitter.twitter_client import TwitterClient\n",
    "from src.text.twitter.twitter_analyze import TwitterSentimentViewer\n",
    "from src.utility.files import ensure_directory\n",
    "\n",
    "authentication_path = Path(\"data\", \"twitter\", \"authentication.json\")\n",
    "ensure_directory(authentication_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "    .output_wrapper button.btn.btn-default,\n",
    "    .output_wrapper .ui-dialog-titlebar {\n",
    "        display: none;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "We need to download and process Wikipedia (only the abstracts) and set up access to Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia\n",
    "\n",
    "Since downloading and processing Wikipedia takes some time we'll start it off now and then return to it later. It should take about `EXPECTED_TIME` to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia = Wikipedia(\n",
    "    language=\"English\",\n",
    "    cache_directory_url=\"https://people.compute.dtu.dk/chegr/data-sets/\",\n",
    "    # maximum_number_of_documents=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter\n",
    "\n",
    "1. Create an account on Twitter (if you don't have any or want one specifically for this course).\n",
    "    * Go to [Twitter](https://twitter.com).\n",
    "\t* You are only have to use this account to *download* tweets.  \n",
    "   \n",
    "2. Create a new app in Twitter Application Management.\n",
    "    * Go to [Twitter's Application Management page](https://apps.twitter.com/).\n",
    "    * Fill out the details:\n",
    "        * Name: Make up a good name for your application.\n",
    "        * Description: \"Application to pull tweets from Twitter for use in DTU CogSys's AI Playground\".\n",
    "        * Website: <https://github.com/DTUComputeCognitiveSystems/AI_playground>\n",
    "\t* Read and agree to [Twitter's developer agreement and policy](https://dev.twitter.com/overview/terms/agreement-and-policy).\n",
    "    \n",
    "3. Copy the consumer key and secret and paste below.\n",
    "\t* In the \"Keys and Access Tokens\" tab under \"Application Settings\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For easier development, the consumer key and secret can be read from a file\n",
    "use_file = True\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "\n",
    "if use_file:\n",
    "    twitter = TwitterClient.authenticate_from_path()\n",
    "    \n",
    "else:\n",
    "    twitter = TwitterClient(consumer_key, consumer_secret)\n",
    "    twitter.save_authentication_to_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We need a simple representation of textâ€¦\n",
    "\n",
    "A bag-of-words representation is a way to represent documents by counting distinct words in each document. First, the distinct words for a set of documents (also called a corpus) is found. This is called the vocabulary. Then for each document the number of occurences of each distinct word is counted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "To show how this is done, we first need a corpus. We start with a fictional corpus of tweets.\n",
    "\n",
    "A new Twitter user with limited vocabulary might tweet something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "fictional_tweets = [\n",
    "    \"This is my first tweet!\",\n",
    "    \"My second tweet is not like my first tweet.\",\n",
    "    \"I really like to tweet in my own words. This is fun.\",\n",
    "    \"I am running out of things to tweet.\",\n",
    "    \"I do not really like to tweet anymore.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the distinct words making up the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fictional_vocabulary = {\n",
    "    word.strip(\".!\").lower()\n",
    "    for tweet in fictional_tweets for word in tweet.split()\n",
    "}\n",
    "display(fictional_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by focusing on the first tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "one_tweet_bag_of_words = BagOfWords(\n",
    "    corpus=fictional_tweets[0],\n",
    "    vocabulary=fictional_vocabulary\n",
    ")\n",
    "one_tweet_bag_of_words.plot_heat_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including the second tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "two_tweet_bag_of_words = BagOfWords(\n",
    "    corpus=fictional_tweets[0:2],\n",
    "    vocabulary=fictional_vocabulary\n",
    ")\n",
    "two_tweet_bag_of_words.plot_heat_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "fictional_bag_of_words = BagOfWords(\n",
    "    corpus=fictional_tweets,\n",
    "    vocabulary=fictional_vocabulary\n",
    ")\n",
    "fictional_bag_of_words.plot_heat_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word cloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fictional_bag_of_words.plot_word_cloud()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf-idf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "fictional_bag_of_words.plot_heat_map(kind=\"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fictional_bag_of_words.plot_word_cloud(kind=\"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. Why not use a set of real tweets instead of the fictional tweets?\n",
    "    * Little overlap between document vocabulary for small sets.\n",
    "2. Try to replacing the fictional tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fetch some tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "tweets = twitter.search(\"summer\", language=\"English\", count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we extract the text from each tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "tweet_texts = [\n",
    "    tweet.text_excluding(\n",
    "        hashtags=False,\n",
    "        mentions=True,\n",
    "        urls=True\n",
    "    )\n",
    "    for tweet in tweets\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary from Wikipedia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_vocabulary = wikipedia.vocabulary\n",
    "display(wikipedia_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both bag-of-words matrix and tf-idf transformed one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "bag_of_words = BagOfWords(\n",
    "    corpus=tweet_texts,\n",
    "    vocabulary=wikipedia_vocabulary\n",
    ")\n",
    "bag_of_words.plot_heat_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words.plot_word_cloud()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words.plot_pca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "bag_of_words.plot_heat_map(kind=\"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words.plot_word_cloud(kind=\"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words.plot_pca(kind=\"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "viewer = TwitterSentimentViewer(twitter_client=twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cells you can write text and have its words highlighted according to their sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "viewer = ViewSentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard = WikipediaSearchDashboard(wikipedia=wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "twittipedia = Twittipedia(twitter_client=twitter, wikipedia=wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "twitter.clear_cache()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nteract": {
   "version": "0.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
