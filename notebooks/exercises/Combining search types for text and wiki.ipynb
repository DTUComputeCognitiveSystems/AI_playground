{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"../global_setup.py\") as setupfile:\n",
    "        exec(setupfile.read())\n",
    "except FileNotFoundError:\n",
    "    print('Setup already completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "from gensim.models.fasttext import FastText\n",
    "from scipy.spatial.distance import cdist\n",
    "import re\n",
    "# ESA relatedness package\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parsed documents.\n",
      "Loading preprocessed documents.\n",
      "Wikipedia loaded.\n"
     ]
    }
   ],
   "source": [
    "from src.text.document_retrieval.wikipedia import Wikipedia # Generic Wikipedia class\n",
    "wikipedia = Wikipedia(\n",
    "    language=\"Danish\",\n",
    "    cache_directory_url=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RsspediaInit:\n",
    "    def __init__(self, wikipedia: Wikipedia, embedding_composition = \"sum\"):\n",
    "        self.embedding_composition = embedding_composition\n",
    "        self.search_results = []\n",
    "        self.content = self.wikipedia_results = None\n",
    "        # Initialize wikipedia\n",
    "        self.wikipedia = wikipedia\n",
    "        # Remove all the line breaks and caret returns from wiki texts\n",
    "        pattern = re.compile('[\\n\\r ]+', re.UNICODE)\n",
    "        self.texts = [self.wikipedia.documents[i].text for i in range(len(self.wikipedia.documents))]\n",
    "        self.texts = [pattern.sub(' ', self.texts[i]) for i in range(len(self.texts))]\n",
    "        self.texts_clean = [self.getCleanWordsList(self.texts[i], return_string = True) for i in range(len(self.texts))]\n",
    "        \n",
    "        # Calculate tf-idf representation for wiki texts (takes time)\n",
    "        self._transformer = TfidfVectorizer(stop_words = None, norm = \"l2\", use_idf = True, sublinear_tf = False)\n",
    "        self._Y = self._transformer.fit_transform(self.texts_clean)\n",
    "        \n",
    "        # Fasttext: initialize the model\n",
    "        bin_path = Path(\"data\", \"fasttext\", \"wiki.da.bin\")\n",
    "        self.model = FastText.load_fasttext_format(str(bin_path))\n",
    "        \n",
    "        # Fasttext: Compute vectorized representation for all wikipedia articles (takes time)\n",
    "        i = 0\n",
    "        i_max = 0\n",
    "        n_removed = 0\n",
    "        self.wikipedia.documents_clean = self.wikipedia.documents.copy()\n",
    "        self.wikipedia_abstract_vectors = []\n",
    "        self.wikipedia_title_vectors = []\n",
    "        pattern1 = re.compile('[^a-zA-Z0-9åÅøØæÆ]+', re.UNICODE)\n",
    "\n",
    "        for n in range(len(self.wikipedia.documents)):\n",
    "            # if abstract length is zero, remove it\n",
    "            try:\n",
    "                if len(pattern1.sub('', self.wikipedia.documents[n].abstract)) == 0:\n",
    "                    del self.wikipedia.documents_clean[n - n_removed]\n",
    "                    n_removed = n_removed + 1\n",
    "                else:\n",
    "                    self.wikipedia_abstract_vectors.append(self.sumVectorRepresentation(text = self.wikipedia.documents[n].abstract, type = self.embedding_composition))\n",
    "                    self.wikipedia_title_vectors.append(self.sumVectorRepresentation(text = self.wikipedia.documents[n].title, type = self.embedding_composition))\n",
    "                    \n",
    "                    i = i + 1\n",
    "                    if i_max > 0 and i > i_max:\n",
    "                        break\n",
    "            except IndexError as e:\n",
    "                print(\"n: {}, n_removed: {}, w.d: {}, w.d_c: {}\".format(n, n_removed, len(self.wikipedia.documents), len(self.wikipedia.documents_clean)))\n",
    "    \n",
    "    def getCleanWordsList(self, text, return_string = False):\n",
    "        pattern = re.compile('[^a-zA-Z0-9åÅøØæÆ ]+', re.UNICODE)\n",
    "        text = pattern.sub('', text)\n",
    "        words = text.lower().strip().split()\n",
    "        words_copy = words.copy()\n",
    "        #stop_words = [\"den\", \"det\", \"en\", \"et\", \"om\", \"for\", \"til\", \"at\", \"af\", \"på\", \"som\", \"og\", \n",
    "        #              \"jeg\", \"mig\", \"mine\", \"min\", \"mit\", \"du\", \"dig\", \"din\", \"dit\", \"dine\", \"han\", \"ham\", \"hun\", \"hende\", \n",
    "        #              \"de\", \"dem\", \"vi\", \"os\", \"sin\", \"sit\", \"sine\", \"sig\"]\n",
    "        \n",
    "        stop_words = [\"den\", \"det\", \"denne\", \"dette\", \"en\", \"et\", \"om\", \"for\", \"til\", \"at\", \"af\", \"på\", \"som\", \"og\", \"er\", \"i\"]\n",
    "        \n",
    "        n_removed = 0\n",
    "        for i in range(len(words)):\n",
    "            if words[i] in stop_words:\n",
    "                words_copy.pop(i - n_removed)\n",
    "                n_removed = n_removed + 1\n",
    "        if return_string:\n",
    "            return ' '.join(words_copy)\n",
    "        else:\n",
    "            return words_copy\n",
    "    \n",
    "    def sumVectorRepresentation(self, text, verbose = False, type = \"sum\"):\n",
    "        # Calculates vectorized represetnation of some text\n",
    "        words = self.getCleanWordsList(text)\n",
    "        text_vector = np.zeros(self.model.wv[\"a\"].shape)\n",
    "        if verbose:\n",
    "            print(\"len: {}, words: {}\".format(len(words), words))\n",
    "        for i in range(len(words)):\n",
    "            try:\n",
    "                if type == \"average\":\n",
    "                    text_vector = text_vector + self.model.wv[words[i]] / len(words)\n",
    "                else: #sum\n",
    "                    text_vector = text_vector + self.model.wv[words[i]]\n",
    "            except KeyError as e:\n",
    "                if verbose:\n",
    "                    print(\"i: {}, e: {}\".format(i, e))\n",
    "                continue\n",
    "        return text_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rsspediainitAVG = RsspediaInit(wikipedia = wikipedia, embedding_composition = \"average\")\n",
    "rsspediainit = RsspediaInit(wikipedia = wikipedia, embedding_composition = \"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rsspedia:\n",
    "    def __init__(self, wikipedia: Wikipedia, rsspediainit: RsspediaInit):\n",
    "        self.rsspediainit = rsspediainit\n",
    "        self.embedding_composition = rsspediainit.embedding_composition\n",
    "        self.search_results = []\n",
    "        self.content = self.wikipedia_results = None\n",
    "        # Initialize wikipedia\n",
    "        self.wikipedia = wikipedia\n",
    "        \n",
    "        self.texts = rsspediainit.texts\n",
    "        \n",
    "        # Calculate tf-idf representation for wiki texts (takes time)\n",
    "        self._transformer = rsspediainit._transformer\n",
    "        self._Y = rsspediainit._Y\n",
    "        \n",
    "        # Fasttext: initialize the model\n",
    "        self.model = rsspediainit.model\n",
    "        \n",
    "        #self.wikipedia.documents_clean \n",
    "        self.wikipedia_abstract_vectors = rsspediainit.wikipedia_abstract_vectors\n",
    "        self.wikipedia_title_vectors = rsspediainit.wikipedia_title_vectors\n",
    "    \n",
    "    def get_ngrams(self, text):\n",
    "        words = self.rsspediainit.getCleanWordsList(text)\n",
    "        words_copy = words.copy()\n",
    "        n_removed = 0\n",
    "        \n",
    "\n",
    "        for i in range(len(words_copy)):\n",
    "            if i > 0:\n",
    "                words_copy.append(words_copy[i - 1] + \" \" + words_copy[i])\n",
    "\n",
    "        return words_copy\n",
    "    \n",
    "    def cdist_func(self, A, B):\n",
    "        # Calculates cosine distance\n",
    "        dists = cdist(A, B, 'cosine')\n",
    "        return np.argmin(dists, axis=0), dists #np.min(dists, axis=0)\n",
    "\n",
    "    def display_beautifully(self, titles, texts, urls):\n",
    "        formatted_result_list = [\"<ol>\"]\n",
    "        for i in range(len(titles)):\n",
    "            formatted_result = \"\\n\".join([\n",
    "                \"<li>\",\n",
    "                f\"<p><a href=\\\"{urls[i]}\\\">{titles[i]}</a></p>\",\n",
    "                f\"<p>{texts[i]}</p>\",\n",
    "                \"</li>\"\n",
    "            ])\n",
    "            formatted_result_list.append(formatted_result)\n",
    "        formatted_result_list.append(\"</ol>\")\n",
    "        formatted_results = \"\\n\".join(formatted_result_list)\n",
    "        return formatted_results\n",
    "\n",
    "    def search_wiki(self, search_texts, n_matches = 3, search_type = \"okapibm25\", remove_similar = False, verbose = False, p = 0.5, d = 0.1):\n",
    "        n_mult_factor = 3 # factor to multiply n_matches with\n",
    "        n_matches = n_matches * 3 # this is done to remove very similar values from the results and ensure we have enough to return\n",
    "        titles = [] \n",
    "        texts = []\n",
    "        urls = []\n",
    "        scores = []\n",
    "        \n",
    "        # (1) Remove unnecessary symbols from the search text\n",
    "        pattern = re.compile('[^a-zA-Z0-9åÅøØæÆ ]+', re.UNICODE)\n",
    "\n",
    "        if search_texts:\n",
    "            for i, text in enumerate(search_texts):\n",
    "                # (1) Remove unnecessary symbols from the search text\n",
    "                text = pattern.sub('', text)\n",
    "                \n",
    "                if search_type == \"okapibm25\":\n",
    "                    wikipedia_results, search_terms = self.wikipedia.search(query = text, k_1 = 1.2, b = 0.75)\n",
    "                    for index, score in wikipedia_results[:n_matches].items():\n",
    "                        document = self.wikipedia.documents[index]\n",
    "                        titles.append(document.title)\n",
    "                        texts.append(document.abstract)\n",
    "                        urls.append(document.url)\n",
    "                        scores.append(score)\n",
    "                elif search_type == \"esa_relatedness\":\n",
    "                    y = self._transformer.transform([text])\n",
    "                    D = np.array((self._Y * y.T).todense())\n",
    "                    indices = np.argsort(-D, axis=0)\n",
    "                    titles = [self.wikipedia.documents[index].title for index in indices[:n_matches, 0]]\n",
    "                    texts = [self.wikipedia.documents[index].abstract for index in indices[:n_matches, 0]]\n",
    "                    urls = [self.wikipedia.documents[index].url for index in indices[:n_matches, 0]]\n",
    "                elif search_type == \"fasttext_a\":\n",
    "                    # Calculate the vectorized representation\n",
    "                    text_vector = self.rsspediainit.sumVectorRepresentation(text = text, type = self.embedding_composition)\n",
    "\n",
    "                    cdist_result = self.cdist_func(self.wikipedia_abstract_vectors, [text_vector])\n",
    "                    cdist_list = cdist_result[1] # List of all the cosine distances\n",
    "                    cdist_list_sorted = np.sort(cdist_list, axis = 0) # Sorted list of cosine distances - to get top N matches\n",
    "\n",
    "                    for i in range(n_matches):\n",
    "                        result = np.where(cdist_list == cdist_list_sorted[i])\n",
    "                        document = self.wikipedia.documents_clean[result[0][0]]\n",
    "                        titles.append(document.title)\n",
    "                        texts.append(document.abstract)\n",
    "                        urls.append(document.url)\n",
    "                        scores.append(cdist_list[result])\n",
    "                elif search_type == \"fasttext_b\":\n",
    "                    ngrams = self.get_ngrams(text)\n",
    "                    r = []\n",
    "                    for i in range(len(ngrams)):\n",
    "                        cdist_result = self.cdist_func(self.wikipedia_title_vectors, [self.rsspediainit.sumVectorRepresentation(text = ngrams[i], type = self.embedding_composition)])\n",
    "                        cdist_result2 = self.cdist_func([self.rsspediainit.sumVectorRepresentation(text = text, type = self.embedding_composition)], [self.rsspediainit.sumVectorRepresentation(text = ngrams[i], type = self.embedding_composition)])\n",
    "\n",
    "                        cdist_list1 = cdist_result[1] # List of all the cosine distances\n",
    "                        cdist_list2 = cdist_result2[1]\n",
    "                        cdist_list = (cdist_list1 * p + cdist_list2 * (1 - p))\n",
    "                        cdist_list_sorted = np.sort(cdist_list, axis = 0) # Sorted list of cosine distances - to get top N matches\n",
    "                        \n",
    "                        for j in range(5):\n",
    "                            x = np.where(cdist_list == cdist_list_sorted[j])[0]\n",
    "                            r.append( (x, cdist_list[x][0]))\n",
    "                            if verbose:\n",
    "                                print(\"{} {} {} {}\".format(x, wikipedia.documents_clean[x[0]].title, cdist_list[x], ngrams[i]))\n",
    "\n",
    "                    # When np.where returns multiple matches, we flatten them\n",
    "                    r_copy = r.copy()\n",
    "                    uniques = []\n",
    "                    for i in range(len(r)-1, -1, -1):\n",
    "                        if len(r[i][0]) > 1:\n",
    "                            r_copy.pop(i)\n",
    "                            for j in range(len(r[i][0])):\n",
    "                                r_copy.append( (np.array([r[i][0][j]]), r[i][1]))\n",
    "\n",
    "                    # Remove duplicate wikipedia pages. They occur because different n-grams can match the same pages\n",
    "                    for i in range(len(r_copy)-1,-1,-1):\n",
    "                        if r_copy[i][0] in uniques:\n",
    "                            r_copy.pop(i)\n",
    "                        else:\n",
    "                            uniques.append(r_copy[i][0])\n",
    "                    \n",
    "                    r = r_copy\n",
    "                    # Transform into list of tuples\n",
    "                    r = [ (r[i][0][0], r[i][1][0]) for i in range(len(r))]\n",
    "                    # Sort the list of tuples by cosine distance\n",
    "                    r = sorted(r, key=lambda tup: tup[1])\n",
    "                    \n",
    "                    for i in range(len(r)):\n",
    "                        document = self.wikipedia.documents_clean[r[i][0]]\n",
    "                        titles.append(document.title)\n",
    "                        #print(\"{} {}\".format(document.title, r[i][1]))\n",
    "                        texts.append(document.abstract)\n",
    "                        urls.append(document.url)\n",
    "                        scores.append(r[i][1])\n",
    "            \n",
    "            if remove_similar:\n",
    "                # Removing too similar search results\n",
    "                # Get vectors of the result titles\n",
    "                title_result_vectors = [self.rsspediainit.sumVectorRepresentation(text = titles[i], type = self.embedding_composition) for i in range(len(titles))]\n",
    "                titles_pruned = titles.copy()\n",
    "                n_removed = 0\n",
    "                ids_removed = []\n",
    "                for i in range(len(titles)):\n",
    "                    # Get cosine distances\n",
    "                    cdist_result = self.cdist_func(title_result_vectors, [self.rsspediainit.sumVectorRepresentation(text = titles[i], type = self.embedding_composition)])[1]\n",
    "                    # Sort cosine distances\n",
    "                    cdist_result_sorted = np.sort(cdist_result, axis = 0)\n",
    "                    rd = []\n",
    "                    for j in range(len(titles) - i):\n",
    "                        if i != j + i:\n",
    "                            x = np.where(cdist_result == cdist_result_sorted[j + i])[0]\n",
    "                            rd.append( (x, cdist_result[x][0]))\n",
    "                            if cdist_result[x][0] < 0.10 and i + j not in ids_removed:\n",
    "                                titles_pruned.pop(i + j - n_removed)\n",
    "                                texts.pop(i + j - n_removed)\n",
    "                                urls.pop(i + j - n_removed)\n",
    "                                scores.pop(i + j - n_removed)\n",
    "                                n_removed = n_removed + 1\n",
    "                                ids_removed.append(i + j)\n",
    "                                #print(\"removed: {}\".format(i + j))\n",
    "                            #print(\"{}-th title: {}, {}-th title: {}, dist: {}\".format(i, titles[i], j + i, titles[j + i], cdist_result[x]))\n",
    "                titles = titles_pruned[:int(n_matches / n_mult_factor)]\n",
    "        return titles[:int(n_matches / n_mult_factor)], texts[:int(n_matches / n_mult_factor)], urls[:int(n_matches / n_mult_factor)], scores[:int(n_matches / n_mult_factor)]\n",
    "\n",
    "\n",
    "#rsspedia = Rsspedia(wikipedia, rsspediainitAVG)\n",
    "rsspedia = Rsspedia(wikipedia, rsspediainit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beredskabsstyrelsen Højskole',\n",
      " 'Sexchikane',\n",
      " 'Beredskabsstyrelsen Teknisk Skole',\n",
      " 'Division (regnskab)',\n",
      " 'Google Apps for Work',\n",
      " 'Knol',\n",
      " 'Civitas A/S',\n",
      " 'Google Hangouts',\n",
      " 'Google',\n",
      " 'Google Search Console']\n",
      "['Google Apps for Work',\n",
      " 'Google',\n",
      " 'Google Briller',\n",
      " 'Google Books',\n",
      " 'Whispering Voices',\n",
      " 'Primærrute 13',\n",
      " 'Gustav Fechner',\n",
      " 'Google Video',\n",
      " 'Globus',\n",
      " 'Google Docs']\n",
      "['Watch Medier',\n",
      " 'Bladkompagniet',\n",
      " 'Finansforbundet',\n",
      " 'Associated Press',\n",
      " 'Jobindex',\n",
      " 'Skandiaweb',\n",
      " 'DELACOUR',\n",
      " 'WarnerMedia',\n",
      " 'Dansk Journalistforbund',\n",
      " 'Deloitte']\n",
      "['Sexchikane',\n",
      " 'Google',\n",
      " 'Google+',\n",
      " 'Google Hangouts',\n",
      " 'Google (søgemaskine)',\n",
      " 'Google Briller',\n",
      " 'Medarbejder',\n",
      " 'Servicemedarbejder',\n",
      " 'Chikane',\n",
      " 'Medarbejdende ægtefælle']\n"
     ]
    }
   ],
   "source": [
    "#test_headline = \"FN's verdensmål sættes i centrum på Folkemødet\"\n",
    "test_headline = \"Google fyrer 13 chefer og 35 medarbejdere for sexchikane\"\n",
    "#test_headline = \"Søren Hansen om den »nøgne sandhed«: Erdogan går på diplomatiske listefødder for at forbedre sit forhold til USA\"\n",
    "#test_headline = \"Søren Hansen\"\n",
    "titles, texts, urls, scores = rsspedia.search_wiki(search_texts = [test_headline], n_matches = 10, search_type = \"okapibm25\")\n",
    "pprint.pprint(titles)\n",
    "\n",
    "titles, texts, urls, scores = rsspedia.search_wiki(search_texts = [test_headline], n_matches = 10, search_type = \"esa_relatedness\")\n",
    "pprint.pprint(titles)\n",
    "\n",
    "titles, texts, urls, scores = rsspedia.search_wiki(search_texts = [test_headline], n_matches = 10, search_type = \"fasttext_a\")\n",
    "pprint.pprint(titles)\n",
    "\n",
    "titles, texts, urls, scores = rsspedia.search_wiki(search_texts = [test_headline], n_matches = 10, search_type = \"fasttext_b\")\n",
    "pprint.pprint(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, texts, urls, scores = rsspedia.search_wiki(search_texts = [test_headline], n_matches = 5, search_type = \"fasttext_b\", p = 0.4)\n",
    "pprint.pprint(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_headline = \"FN's verdensmål sættes i centrum på Folkemødet\"\n",
    "#test_headline = \"Google fyrer 13 chefer og 35 medarbejdere for sexchikane\"\n",
    "#test_headline = \"Søren Hansen om den »nøgne sandhed«: Erdogan går på diplomatiske listefødder for at forbedre sit forhold til USA\"\n",
    "\n",
    "p = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "test_headlines = [\"Justin Bieber elsker burger\",\n",
    "                  \"Søren Hansen sover godt\"]\n",
    "\n",
    "for i in range(len(p)):\n",
    "    for j in range(len(test_headlines)):\n",
    "        titles, texts, urls, scores = rsspedia.search_wiki(search_texts = [test_headlines[j]], n_matches = 15, search_type = \"fasttext_b\", p = p[i], verbose = False)\n",
    "        print(\"*** ({}) {} ***\".format(p[i], test_headlines[j]))\n",
    "        [print(\"({}) {:.5f} {}\".format(k + 1, scores[k], titles[k])) for k in range(len(titles))]\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsspedia.cdist_func([rsspedia.wikipedia_title_vectors[44919]], [rsspedia.sumVectorRepresentation(\"sørEN\")])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_headline = \"FN's verdensmål sættes i centrum på Folkemødet\"\n",
    "#test_headline = \"Google fyrer 13 chefer og 35 medarbejdere for sexchikane\"\n",
    "#test_headline = \"Søren Hansen om den »nøgne sandhed«: Erdogan går på diplomatiske listefødder for at forbedre sit forhold til USA\"\n",
    "\n",
    "d = [0.01, 0.05, 0.1, 0.2]\n",
    "\n",
    "test_headlines = [\"Justin Bieber elsker burger\",\n",
    "                  \"Søren Hansen sover godt\"]\n",
    "\n",
    "for i in range(len(d)):\n",
    "    for j in range(len(test_headlines)):\n",
    "        titles, texts, urls, scores = rsspedia.search_wiki(search_texts = [test_headlines[j]], n_matches = 15, search_type = \"fasttext_b\", d = d[i], verbose = False)\n",
    "        print(\"*** ({}) {} ***\".format(d[i], test_headlines[j]))\n",
    "        [print(\"({}) {:.5f} {}\".format(k + 1, scores[k], titles[k])) for k in range(len(titles))]\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.28305321e+00,  2.55240115e+00, -1.45726916e+00, -8.23756914e+00,\n",
       "       -8.64112043e+00, -6.47998441e+00, -1.29929687e+00, -9.55795763e-01,\n",
       "        1.65495173e+01, -1.69481530e+01, -1.36593221e+01, -5.81666787e+00,\n",
       "       -3.23517251e+00,  2.63173723e+01,  1.38813533e+01,  7.31259770e+00,\n",
       "        1.57536662e+01,  2.09791069e+01, -5.90013766e+00, -3.35569414e+00,\n",
       "        1.15768945e+01,  1.14082091e+01, -3.92751399e+00, -1.80111527e+01,\n",
       "        7.23412335e+00, -4.99937767e+00, -1.94241644e+01, -1.12354711e+01,\n",
       "       -1.51957655e+01, -1.76585835e+01, -1.45289562e+01,  1.16943159e+01,\n",
       "       -7.11829895e+00,  5.23034242e+00, -8.89993020e+00,  1.65516643e-01,\n",
       "        6.65601367e+00, -2.77061326e+00,  1.27500313e+01, -9.06576821e+00,\n",
       "        1.39543985e+00,  2.43877220e+01, -9.57371298e+00,  6.56653776e+00,\n",
       "       -8.08751804e+00, -9.48655071e+00,  1.14961673e+01, -9.75480168e+00,\n",
       "       -1.77268107e+01, -5.30189526e+00,  2.77642972e-01,  4.23708503e+00,\n",
       "        7.65727830e+00, -9.80253241e+00,  1.37540741e+01, -6.49794550e+00,\n",
       "        1.40365870e+01, -1.58395714e+01, -3.14514865e+00,  1.90265925e-01,\n",
       "       -2.08466919e+01,  2.24976984e+01,  2.64928200e+01,  9.37179471e+00,\n",
       "       -3.45673574e+01, -1.48579855e+01,  1.18243599e+01, -5.09226892e+00,\n",
       "       -4.62699470e+00,  3.52775147e-02,  2.79207357e+00, -6.37845062e+00,\n",
       "       -2.93134827e+00, -3.34937243e-01, -6.89369741e+00,  1.27601764e+01,\n",
       "        6.64147395e+00,  7.28603030e-01, -3.14596360e-01, -9.98467097e+00,\n",
       "       -1.11115607e+01,  5.02127878e-01,  9.85198990e+00, -2.22317917e+01,\n",
       "       -3.52659898e-01, -1.19004116e+01,  1.29698089e+01,  6.42302465e+00,\n",
       "       -6.47521980e+00,  1.70782829e+01,  1.52575342e+01,  9.18572737e+00,\n",
       "        1.78952801e+01,  9.06948538e+00, -2.28678455e+01,  5.09292031e-01,\n",
       "       -2.01967657e+01,  9.30146383e+00,  5.43316867e+00,  3.61964318e+00,\n",
       "        9.78182378e+00, -2.78618706e+00, -5.34383226e-01, -1.71179879e+00,\n",
       "       -1.61694153e+01, -2.86332269e+00, -2.02123868e+01, -2.18140105e+00,\n",
       "        1.23362282e+01,  7.93954731e+00,  3.88900732e-01, -1.56774568e+01,\n",
       "        1.49035118e+00, -8.48236494e+00, -9.63041961e+00,  1.96590821e+00,\n",
       "        4.41897112e+00,  5.32872768e+00, -6.18873903e+00, -5.56611454e+00,\n",
       "       -1.06949653e+01,  1.45861620e+01, -1.71766892e+01, -1.32348724e+01,\n",
       "       -2.61629138e+00, -7.48401875e+00,  7.90394989e+00, -5.10036524e+00,\n",
       "        1.88995946e-01,  4.22781276e+00,  1.73547227e+01, -5.65804598e+00,\n",
       "       -1.39388440e+01, -2.15606097e+01,  9.87381339e+00,  9.90034146e-01,\n",
       "        1.13753857e+01,  8.59380043e+00, -4.32440967e+00, -1.89930730e+00,\n",
       "        1.03350738e+01,  1.40398291e+01,  1.36780704e+00,  1.24302519e+01,\n",
       "        7.65340858e+00, -1.31226164e+01,  2.16409208e+01,  7.18728532e+00,\n",
       "        4.10133254e+00, -2.45901584e+00,  6.30404110e+00,  8.84961426e+00,\n",
       "       -5.60371616e+00, -6.63132660e+00, -1.08485258e+01,  1.95975089e+00,\n",
       "        1.12672997e+01, -1.87019574e+01, -7.69140818e-02,  8.79066024e+00,\n",
       "       -6.72487861e+00,  4.02909694e+00, -1.24491940e+01,  9.42915827e+00,\n",
       "       -8.41432357e+00, -3.83636271e+00,  3.80924613e+00, -2.01936277e+00,\n",
       "        2.22375664e+00, -5.79722864e+00, -4.84465616e+00,  2.26695902e+00,\n",
       "        1.16279992e+01,  4.73592122e+00,  4.28385477e+00, -1.03572814e+01,\n",
       "       -4.93256718e+00, -9.43453256e-02, -5.06793965e-01,  3.75083894e+00,\n",
       "        5.48054073e+00, -1.38947750e+01,  7.03491078e+00,  1.12605587e+01,\n",
       "        1.45660817e+01, -8.36174249e-01, -1.21852318e+01, -6.60589102e+00,\n",
       "       -1.08919892e+00, -8.63824270e+00,  5.13001136e+00, -3.29087858e+00,\n",
       "        4.50274788e+00, -1.17379025e+01, -5.95457411e+00, -1.09509485e+01,\n",
       "       -1.79405745e+00,  3.19767513e+00, -9.82877729e-01, -2.74744915e-01,\n",
       "        6.25101379e+00, -2.42437380e+00, -1.05524878e+01,  1.02344362e+01,\n",
       "        4.45407777e+00,  1.88800007e+00, -1.81645410e+00,  1.42941320e+01,\n",
       "       -7.04012327e-01, -2.36787900e+00, -3.44920164e+00,  9.95034260e-01,\n",
       "        1.56485122e+01,  4.31901991e+00,  1.36170655e+01,  9.57774936e+00,\n",
       "        6.94450986e+00,  4.75303150e-01, -6.75476653e-01,  1.26774613e+01,\n",
       "       -2.04209664e-01, -3.14233902e-01, -4.43396494e+00, -5.74911368e-01,\n",
       "        8.81141773e+00,  4.34189611e+00,  8.10828331e+00, -9.99863603e+00,\n",
       "        6.06159946e+00, -1.48220342e+01,  1.47789100e+01,  1.17657501e+01,\n",
       "       -2.78404750e+00, -1.68314795e+00, -1.80719851e+00,  1.27801354e+00,\n",
       "        1.07904630e+00, -5.50367284e+00, -1.13943183e+00, -2.54577995e+00,\n",
       "        1.14737072e+01, -3.03263411e+00,  7.93517643e+00,  9.40296157e+00,\n",
       "        1.78397018e+01, -1.23221768e+01,  1.82669948e+00,  1.92677837e+00,\n",
       "       -2.71332078e+00,  1.54519075e+01,  3.72314642e+00, -5.27956762e+00,\n",
       "       -1.78274441e+00, -1.59976781e+01,  4.95038154e+00,  4.80505299e-01,\n",
       "        5.08756258e+00,  4.61089612e+00, -3.32240755e+00,  1.76157430e+00,\n",
       "        2.84251380e+00, -9.09683172e+00, -4.11195451e+00, -1.43631072e+01,\n",
       "       -6.83178225e+00,  8.34486971e+00,  5.68444814e+00, -1.85030308e+00,\n",
       "        1.99285770e+00,  5.10351214e+00, -1.19587199e-01, -9.98200917e+00,\n",
       "        6.95784096e+00, -2.13112848e+00, -4.74968054e+00, -5.66338256e-01,\n",
       "       -1.71330867e+01,  9.34606765e+00,  3.05885276e+00, -1.61379807e+01,\n",
       "        4.25158413e+00,  5.75693028e+00,  4.54288569e+00, -1.17091809e+01,\n",
       "        6.66723819e-01,  1.90932472e-01,  2.71660082e+00, -4.04290632e+00,\n",
       "       -2.43162355e+00, -1.90753326e-02, -1.28401308e+01,  5.36366580e+00,\n",
       "        1.03557492e+01, -3.81582939e+00,  1.55526245e+01,  2.42662335e+00,\n",
       "        4.60609707e+00, -5.09670276e+00, -8.43317588e+00, -4.48586028e+00])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsspedia.wikipedia_abstract_vectors[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05014786,  0.32823414, -0.2863861 , -0.14378658, -0.20674805,\n",
       "        0.14268966,  0.25108913,  0.31807059, -0.45580789, -0.54190654,\n",
       "       -0.4586145 ,  0.20334904, -0.12572755,  0.4007116 ,  0.02855139,\n",
       "       -0.24636118,  0.08269521, -0.43348765, -0.09566914,  0.20010217,\n",
       "       -0.14064902,  0.45069507, -0.09785812, -0.13032831,  0.5983457 ,\n",
       "        0.16919152, -0.2981205 ,  0.60975236, -0.26399106, -0.08436299,\n",
       "       -0.46794125,  0.31169823,  0.58768255,  0.02459755,  0.0424697 ,\n",
       "        0.40876111,  0.00973499, -0.29088885,  0.60246372, -0.33718371,\n",
       "       -0.12872784,  1.03731573, -0.47151724, -0.37271887, -0.48083046,\n",
       "       -0.70766419, -0.14369157, -0.10628562,  0.15250066, -0.73531884,\n",
       "       -0.28511685, -0.5384478 ,  0.22355257, -0.28713101, -0.44965044,\n",
       "       -0.60707319,  0.11590097, -0.1102048 , -0.0708666 , -0.09127642,\n",
       "       -0.14107014,  0.166862  , -0.01928574,  0.06991053, -0.04413012,\n",
       "        0.10818345, -0.03567166, -0.02626268,  0.23833179, -0.41599247,\n",
       "       -0.09364146,  0.26654196,  0.61256832, -0.24447298, -0.13656043,\n",
       "       -0.06297645,  0.356159  , -0.7650879 ,  0.46272865, -0.06877378,\n",
       "       -0.21919076,  0.10054684, -0.24655369, -0.06386452,  0.24773237,\n",
       "        0.46446046,  0.40619287,  0.12165523, -0.03890087,  0.18066303,\n",
       "       -0.49888271, -0.82861966, -0.08937507, -0.14916858, -0.38684353,\n",
       "        0.11556768, -0.22797756, -0.11301004,  0.007287  ,  0.01841568,\n",
       "       -0.31487384, -0.1935437 , -0.28206006, -0.2786949 , -0.45907634,\n",
       "        0.15255301,  0.01891741,  0.01887859,  0.46413538,  0.40738007,\n",
       "        0.43735471, -0.16038778, -0.28783852,  0.0036921 , -0.24102266,\n",
       "        0.06338135,  0.05605609, -0.10026233, -0.20742951, -0.57706755,\n",
       "        0.10406264,  0.06603687, -0.24742195, -0.45989949, -0.03690191,\n",
       "       -0.2992985 , -0.02758828, -0.20360847,  0.63264161,  0.29224211,\n",
       "        0.48442501, -0.46608955,  0.2593734 ,  0.03028627,  0.22179376,\n",
       "        0.19508158, -0.61737758, -0.19064912,  0.40994778, -0.13629808,\n",
       "        0.25468194,  0.21729115, -0.69657612,  0.04089867, -0.14637616,\n",
       "       -0.79659396,  0.07246087,  0.25041774, -0.40279737,  0.55700481,\n",
       "       -0.20890124, -0.13789736,  0.64885658, -0.76018625, -0.51055616,\n",
       "        0.45601764,  0.32092124, -0.42869574, -0.05779615,  0.4472563 ,\n",
       "       -0.05685443,  0.56100231,  0.01715616,  0.03098719, -0.05141116,\n",
       "        0.05756958,  0.7240572 ,  0.27808172,  0.12106722,  0.69815105,\n",
       "       -0.05171631,  0.41235369,  0.28774315,  0.16424763, -0.22284923,\n",
       "       -0.39550453, -0.4694714 , -0.04044255, -0.06121159,  0.31939587,\n",
       "       -0.1559654 ,  0.25703833, -0.24162157,  0.49010414, -0.29881987,\n",
       "       -0.14730461, -0.60575211, -0.79578179, -0.02254447, -0.49396196,\n",
       "       -0.21055304, -0.47756037, -0.06389265,  0.15717618, -1.06709194,\n",
       "       -0.44358796, -0.76174384,  0.32506099,  0.56481749,  0.02903103,\n",
       "        0.11100552, -0.08801954, -0.09721865,  0.07028275,  0.06978223,\n",
       "       -0.7121591 ,  0.4623813 , -0.15317631, -0.25649169, -0.0594447 ,\n",
       "       -0.36726326,  0.19330396,  0.18670113,  0.06416171, -0.34942707,\n",
       "        0.05038854,  0.28524274,  0.31381163, -0.3591179 , -0.12105781,\n",
       "        0.36270514,  0.56798089,  0.00237881,  0.34506187,  0.5283919 ,\n",
       "       -0.0113251 ,  0.36616543, -0.48435944, -0.05333912, -0.41188523,\n",
       "        0.05028031,  0.29551488, -0.60914975,  0.19383939, -0.04677805,\n",
       "        0.01576805,  0.38870618, -0.41340256,  0.00673379,  0.30460289,\n",
       "       -0.27749428, -0.33054534,  0.33766025, -0.57080281,  0.74016076,\n",
       "        0.15950575, -0.55862206,  0.18946324,  0.01264311,  0.34354529,\n",
       "        0.23523958, -0.1454502 , -0.23570189, -0.63724595,  0.26260835,\n",
       "        0.41624552, -0.12113284,  0.37754369,  0.2845411 ,  0.53800166,\n",
       "        0.21391881, -0.68440616, -0.32493836, -0.83821291, -0.58439952,\n",
       "       -0.26767781,  0.14335634, -0.63824272, -0.15193172,  0.45361882,\n",
       "       -0.04232269,  0.09408458, -0.36319417, -0.2730065 , -0.39867663,\n",
       "        0.30729863, -0.36734328, -0.22349162, -0.37613896, -0.76015598,\n",
       "       -0.04058664,  0.29646614,  0.11295611, -0.29022428,  0.08769076,\n",
       "       -0.13310841,  0.12980691, -0.38625056,  0.46255416, -0.53581989,\n",
       "       -0.49846229, -0.17441355, -0.43059382,  0.06805667,  0.16047744,\n",
       "       -0.56948471,  0.27957305, -0.0286237 ,  0.14416333,  0.31318688])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsspediainit.sumVectorRepresentation(\"bug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
