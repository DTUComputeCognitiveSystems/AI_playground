{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"../global_setup.py\") as setupfile:\n",
    "        exec(setupfile.read())\n",
    "except FileNotFoundError:\n",
    "    print('Setup already completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "from gensim.models.fasttext import FastText\n",
    "from scipy.spatial.distance import cdist\n",
    "import re\n",
    "# ESA relatedness package\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.text.document_retrieval.wikipedia import Wikipedia # Generic Wikipedia class\n",
    "wikipedia = Wikipedia(\n",
    "    language=\"Danish\",\n",
    "    cache_directory_url=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RsspediaInit:\n",
    "    def __init__(self, wikipedia: Wikipedia, embedding_composition = \"sum\"):\n",
    "        self.embedding_composition = embedding_composition\n",
    "        self.search_results = []\n",
    "        self.content = self.wikipedia_results = None\n",
    "        # Initialize wikipedia\n",
    "        self.wikipedia = wikipedia\n",
    "        # Remove all the line breaks and caret returns from wiki texts\n",
    "        pattern = re.compile('[\\n\\r ]+', re.UNICODE)\n",
    "        self.texts = [self.wikipedia.documents[i].text for i in range(len(self.wikipedia.documents))]\n",
    "        self.texts = [pattern.sub(' ', self.texts[i]) for i in range(len(self.texts))]\n",
    "        self.texts_clean = [self.getCleanWordsList(self.texts[i], return_string = True) for i in range(len(self.texts))]\n",
    "        \n",
    "        # Calculate tf-idf representation for wiki texts (takes time)\n",
    "        self._transformer = TfidfVectorizer(stop_words = None, norm = \"l2\", use_idf = True, sublinear_tf = False)\n",
    "        self._Y = self._transformer.fit_transform(self.texts_clean)\n",
    "        \n",
    "        # Fasttext: initialize the model\n",
    "        bin_path = Path(\"data\", \"fasttext\", \"wiki.da.bin\")\n",
    "        self.model = FastText.load_fasttext_format(str(bin_path))\n",
    "        \n",
    "        # Fasttext: Compute vectorized representation for all wikipedia articles (takes time)\n",
    "        i = 0\n",
    "        i_max = 0\n",
    "        n_removed = 0\n",
    "        self.wikipedia.documents_clean = self.wikipedia.documents.copy()\n",
    "        self.wikipedia_abstract_vectors = []\n",
    "        self.wikipedia_title_vectors = []\n",
    "        pattern1 = re.compile('[^a-zA-Z0-9åÅøØæÆ]+', re.UNICODE)\n",
    "\n",
    "        for n in range(len(self.wikipedia.documents)):\n",
    "            # if abstract length is zero, remove it\n",
    "            try:\n",
    "                if len(pattern1.sub('', self.wikipedia.documents[n].abstract)) == 0:\n",
    "                    del self.wikipedia.documents_clean[n - n_removed]\n",
    "                    n_removed = n_removed + 1\n",
    "                else:\n",
    "                    self.wikipedia_abstract_vectors.append(self.sumVectorRepresentation(text = self.wikipedia.documents[n].abstract, type = self.embedding_composition))\n",
    "                    self.wikipedia_title_vectors.append(self.sumVectorRepresentation(text = self.wikipedia.documents[n].title, type = self.embedding_composition))\n",
    "                    \n",
    "                    i = i + 1\n",
    "                    if i_max > 0 and i > i_max:\n",
    "                        break\n",
    "            except IndexError as e:\n",
    "                print(\"n: {}, n_removed: {}, w.d: {}, w.d_c: {}\".format(n, n_removed, len(self.wikipedia.documents), len(self.wikipedia.documents_clean)))\n",
    "    \n",
    "    def getCleanWordsList(self, text, return_string = False):\n",
    "        pattern = re.compile('[^a-zA-Z0-9åÅøØæÆ ]+', re.UNICODE)\n",
    "        text = pattern.sub('', text)\n",
    "        words = text.lower().strip().split()\n",
    "        words_copy = words.copy()\n",
    "        #stop_words = [\"den\", \"det\", \"en\", \"et\", \"om\", \"for\", \"til\", \"at\", \"af\", \"på\", \"som\", \"og\", \n",
    "        #              \"jeg\", \"mig\", \"mine\", \"min\", \"mit\", \"du\", \"dig\", \"din\", \"dit\", \"dine\", \"han\", \"ham\", \"hun\", \"hende\", \n",
    "        #              \"de\", \"dem\", \"vi\", \"os\", \"sin\", \"sit\", \"sine\", \"sig\"]\n",
    "        \n",
    "        stop_words = [\"den\", \"det\", \"denne\", \"dette\", \"en\", \"et\", \"om\", \"for\", \"til\", \"at\", \"af\", \"på\", \"som\", \"og\", \"er\"]\n",
    "        \n",
    "        n_removed = 0\n",
    "        for i in range(len(words)):\n",
    "            if words[i] in stop_words:\n",
    "                words_copy.pop(i - n_removed)\n",
    "                n_removed = n_removed + 1\n",
    "        if return_string:\n",
    "            return ' '.join(words_copy)\n",
    "        else:\n",
    "            return words_copy\n",
    "    \n",
    "    def sumVectorRepresentation(self, text, verbose = False, type = \"sum\"):\n",
    "        # Calculates vectorized represetnation of some text\n",
    "        words = self.getCleanWordsList(text)\n",
    "        text_vector = np.zeros(self.model.wv[\"a\"].shape)\n",
    "        if verbose:\n",
    "            print(\"len: {}, words: {}\".format(len(words), words))\n",
    "        for i in range(len(words)):\n",
    "            try:\n",
    "                if type == \"average\":\n",
    "                    text_vector = text_vector + self.model.wv[words[i]] / len(words)\n",
    "                else: #sum\n",
    "                    text_vector = text_vector + self.model.wv[words[i]]\n",
    "            except KeyError as e:\n",
    "                if verbose:\n",
    "                    print(\"i: {}, e: {}\".format(i, e))\n",
    "                continue\n",
    "        return text_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rsspediainitAVG = RsspediaInit(wikipedia = wikipedia, embedding_composition = \"average\")\n",
    "rsspediainit = RsspediaInit(wikipedia = wikipedia, embedding_composition = \"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rsspedia:\n",
    "    def __init__(self, wikipedia: Wikipedia, rsspediainit: RsspediaInit):\n",
    "        self.rsspediainit = rsspediainit\n",
    "        self.embedding_composition = rsspediainit.embedding_composition\n",
    "        self.search_results = []\n",
    "        self.content = self.wikipedia_results = None\n",
    "        # Initialize wikipedia\n",
    "        self.wikipedia = wikipedia\n",
    "        \n",
    "        self.texts = rsspediainit.texts\n",
    "        \n",
    "        # Calculate tf-idf representation for wiki texts (takes time)\n",
    "        self._transformer = rsspediainit._transformer\n",
    "        self._Y = rsspediainit._Y\n",
    "        \n",
    "        # Fasttext: initialize the model\n",
    "        self.model = rsspediainit.model\n",
    "        \n",
    "        #self.wikipedia.documents_clean \n",
    "        self.wikipedia_abstract_vectors = rsspediainit.wikipedia_abstract_vectors\n",
    "        self.wikipedia_title_vectors = rsspediainit.wikipedia_title_vectors\n",
    "    \n",
    "    def get_ngrams(self, text):\n",
    "        words = self.rsspediainit.getCleanWordsList(text)\n",
    "        words_copy = words.copy()\n",
    "        n_removed = 0\n",
    "        \n",
    "\n",
    "        for i in range(len(words_copy)):\n",
    "            if i > 0:\n",
    "                words_copy.append(words_copy[i - 1] + \" \" + words_copy[i])\n",
    "\n",
    "        return words_copy\n",
    "    \n",
    "    def cdist_func(self, A, B):\n",
    "        # Calculates cosine distance\n",
    "        dists = cdist(A, B, 'cosine')\n",
    "        return np.argmin(dists, axis=0), dists #np.min(dists, axis=0)\n",
    "\n",
    "    def display_beautifully(self, titles, texts, urls):\n",
    "        formatted_result_list = [\"<ol>\"]\n",
    "        for i in range(len(titles)):\n",
    "            formatted_result = \"\\n\".join([\n",
    "                \"<li>\",\n",
    "                f\"<p><a href=\\\"{urls[i]}\\\">{titles[i]}</a></p>\",\n",
    "                f\"<p>{texts[i]}</p>\",\n",
    "                \"</li>\"\n",
    "            ])\n",
    "            formatted_result_list.append(formatted_result)\n",
    "        formatted_result_list.append(\"</ol>\")\n",
    "        formatted_results = \"\\n\".join(formatted_result_list)\n",
    "        return formatted_results\n",
    "\n",
    "    def search_wiki(self, search_texts, n_matches = 3, search_type = \"okapibm25\", remove_similar = False, verbose = False, p = 0.5):\n",
    "        n_mult_factor = 3 # factor to multiply n_matches with\n",
    "        n_matches = n_matches * 3 # this is done to remove very similar values from the results and ensure we have enough to return\n",
    "        titles = [] \n",
    "        texts = []\n",
    "        urls = []\n",
    "        scores = []\n",
    "        \n",
    "        # (1) Remove unnecessary symbols from the search text\n",
    "        pattern = re.compile('[^a-zA-Z0-9åÅøØæÆ ]+', re.UNICODE)\n",
    "\n",
    "        if search_texts:\n",
    "            for i, text in enumerate(search_texts):\n",
    "                # (1) Remove unnecessary symbols from the search text\n",
    "                text = pattern.sub('', text)\n",
    "                \n",
    "                if search_type == \"okapibm25\":\n",
    "                    wikipedia_results, search_terms = self.wikipedia.search(query = text, k_1 = 1.2, b = 0.75)\n",
    "                    for index, score in wikipedia_results[:n_matches].items():\n",
    "                        document = self.wikipedia.documents[index]\n",
    "                        titles.append(document.title)\n",
    "                        texts.append(document.abstract)\n",
    "                        urls.append(document.url)\n",
    "                        scores.append(score)\n",
    "                elif search_type == \"esa_relatedness\":\n",
    "                    y = self._transformer.transform([text])\n",
    "                    D = np.array((self._Y * y.T).todense())\n",
    "                    indices = np.argsort(-D, axis=0)\n",
    "                    titles = [self.wikipedia.documents[index].title for index in indices[:n_matches, 0]]\n",
    "                    texts = [self.wikipedia.documents[index].abstract for index in indices[:n_matches, 0]]\n",
    "                    urls = [self.wikipedia.documents[index].url for index in indices[:n_matches, 0]]\n",
    "                elif search_type == \"fasttext_a\":\n",
    "                    # Calculate the vectorized representation\n",
    "                    text_vector = self.rsspediainit.sumVectorRepresentation(text = text, type = self.embedding_composition)\n",
    "\n",
    "                    cdist_result = self.cdist_func(self.wikipedia_abstract_vectors, [text_vector])\n",
    "                    cdist_list = cdist_result[1] # List of all the cosine distances\n",
    "                    cdist_list_sorted = np.sort(cdist_list, axis = 0) # Sorted list of cosine distances - to get top N matches\n",
    "\n",
    "                    for i in range(n_matches):\n",
    "                        result = np.where(cdist_list == cdist_list_sorted[i])\n",
    "                        document = self.wikipedia.documents_clean[result[0][0]]\n",
    "                        titles.append(document.title)\n",
    "                        texts.append(document.abstract)\n",
    "                        urls.append(document.url)\n",
    "                        scores.append(cdist_list[result])\n",
    "                elif search_type == \"fasttext_b\":\n",
    "                    ngrams = self.get_ngrams(text)\n",
    "                    r = []\n",
    "                    for i in range(len(ngrams)):\n",
    "                        cdist_result = self.cdist_func(self.wikipedia_title_vectors, [self.rsspediainit.sumVectorRepresentation(text = ngrams[i], type = self.embedding_composition)])\n",
    "                        cdist_result2 = self.cdist_func([self.rsspediainit.sumVectorRepresentation(text = text, type = self.embedding_composition)], [self.rsspediainit.sumVectorRepresentation(text = ngrams[i], type = self.embedding_composition)])\n",
    "\n",
    "                        cdist_list1 = cdist_result[1] # List of all the cosine distances\n",
    "                        cdist_list2 = cdist_result2[1]\n",
    "                        cdist_list = (cdist_list1 * p + cdist_list2 * (1 - p))\n",
    "                        cdist_list_sorted = np.sort(cdist_list, axis = 0) # Sorted list of cosine distances - to get top N matches\n",
    "                        \n",
    "                        for j in range(5):\n",
    "                            x = np.where(cdist_list == cdist_list_sorted[j])[0]\n",
    "                            r.append( (x, cdist_list[x][0]))\n",
    "                            if verbose:\n",
    "                                print(\"{} {} {} {}\".format(x, wikipedia.documents_clean[x[0]].title, cdist_list[x], ngrams[i]))\n",
    "\n",
    "                    # When np.where returns multiple matches, we flatten them\n",
    "                    r_copy = r.copy()\n",
    "                    uniques = []\n",
    "                    for i in range(len(r)-1, -1, -1):\n",
    "                        if len(r[i][0]) > 1:\n",
    "                            r_copy.pop(i)\n",
    "                            for j in range(len(r[i][0])):\n",
    "                                r_copy.append( (np.array([r[i][0][j]]), r[i][1]))\n",
    "\n",
    "                    # Remove duplicate wikipedia pages. They occur because different n-grams can match the same pages\n",
    "                    for i in range(len(r_copy)-1,-1,-1):\n",
    "                        if r_copy[i][0] in uniques:\n",
    "                            r_copy.pop(i)\n",
    "                        else:\n",
    "                            uniques.append(r_copy[i][0])\n",
    "                    \n",
    "                    r = r_copy\n",
    "                    # Transform into list of tuples\n",
    "                    r = [ (r[i][0][0], r[i][1][0]) for i in range(len(r))]\n",
    "                    # Sort the list of tuples by cosine distance\n",
    "                    r = sorted(r, key=lambda tup: tup[1])\n",
    "                    \n",
    "                    for i in range(len(r)):\n",
    "                        document = self.wikipedia.documents_clean[r[i][0]]\n",
    "                        titles.append(document.title)\n",
    "                        #print(\"{} {}\".format(document.title, r[i][1]))\n",
    "                        texts.append(document.abstract)\n",
    "                        urls.append(document.url)\n",
    "                        scores.append(r[i][1])\n",
    "            \n",
    "            if remove_similar:\n",
    "                # Removing too similar search results\n",
    "                # Get vectors of the result titles\n",
    "                title_result_vectors = [self.rsspediainit.sumVectorRepresentation(text = titles[i], type = self.embedding_composition) for i in range(len(titles))]\n",
    "                titles_pruned = titles.copy()\n",
    "                n_removed = 0\n",
    "                ids_removed = []\n",
    "                for i in range(len(titles)):\n",
    "                    # Get cosine distances\n",
    "                    cdist_result = self.cdist_func(title_result_vectors, [self.rsspediainit.sumVectorRepresentation(text = titles[i], type = self.embedding_composition)])[1]\n",
    "                    # Sort cosine distances\n",
    "                    cdist_result_sorted = np.sort(cdist_result, axis = 0)\n",
    "                    rd = []\n",
    "                    for j in range(len(titles) - i):\n",
    "                        if i != j + i:\n",
    "                            x = np.where(cdist_result == cdist_result_sorted[j + i])[0]\n",
    "                            rd.append( (x, cdist_result[x][0]))\n",
    "                            if cdist_result[x][0] < 0.10 and i + j not in ids_removed:\n",
    "                                titles_pruned.pop(i + j - n_removed)\n",
    "                                n_removed = n_removed + 1\n",
    "                                ids_removed.append(i + j)\n",
    "                                #print(\"removed: {}\".format(i + j))\n",
    "                            #print(\"{}-th title: {}, {}-th title: {}, dist: {}\".format(i, titles[i], j + i, titles[j + i], cdist_result[x]))\n",
    "                titles = titles_pruned[:int(n_matches / n_mult_factor)]\n",
    "        return titles, texts, urls, scores\n",
    "\n",
    "\n",
    "#rsspedia = Rsspedia(wikipedia, rsspediainitAVG)\n",
    "rsspedia = Rsspedia(wikipedia, rsspediainitSUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test_headline = \"FN's verdensmål sættes i centrum på Folkemødet\"\n",
    "#test_headline = \"Google fyrer 13 chefer og 35 medarbejdere for sexchikane\"\n",
    "#test_headline = \"Søren Hansen om den »nøgne sandhed«: Erdogan går på diplomatiske listefødder for at forbedre sit forhold til USA\"\n",
    "test_headline = \"Søren Hansen\"\n",
    "titles, texts, urls, scores = rsspedia.search_wiki(search_texts = [test_headline], n_matches = 5, search_type = \"okapibm25\")\n",
    "pprint.pprint(titles)\n",
    "\n",
    "titles, texts, urls, scores = rsspedia.search_wiki(search_texts = [test_headline], n_matches = 5, search_type = \"esa_relatedness\")\n",
    "pprint.pprint(titles)\n",
    "\n",
    "titles, texts, urls, scores = rsspedia.search_wiki(search_texts = [test_headline], n_matches = 5, search_type = \"fasttext_a\")\n",
    "pprint.pprint(titles)\n",
    "\n",
    "titles, texts, urls, scores = rsspedia.search_wiki(search_texts = [test_headline], n_matches = 5, search_type = \"fasttext_b\")\n",
    "pprint.pprint(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, texts, urls, scores = rsspedia.search_wiki(search_texts = [test_headline], n_matches = 5, search_type = \"fasttext_b\", p = 0.4)\n",
    "pprint.pprint(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_headline = \"Søren Hansen om den »nøgne sandhed«: Erdogan går på diplomatiske listefødder for at forbedre sit forhold til USA\"\n",
    "\n",
    "p = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "test_headlines = [\"Justin Bieber elsker burger\",\n",
    "                  \"Søren Hansen sover godt\"]\n",
    "\n",
    "for i in range(len(p)):\n",
    "    for j in range(len(test_headlines)):\n",
    "        titles, texts, urls, scores = rsspedia.search_wiki(search_texts = [test_headlines[j]], n_matches = 15, search_type = \"fasttext_b\", p = p[i], verbose = False)\n",
    "        print(\"*** ({}) {} ***\".format(p[i], test_headlines[j]))\n",
    "        [print(\"({}) {:.5f} {}\".format(k + 1, scores[k], titles[k])) for k in range(len(titles))]\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsspedia.cdist_func([rsspedia.wikipedia_title_vectors[44919]], [rsspedia.sumVectorRepresentation(\"sørEN\")])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
