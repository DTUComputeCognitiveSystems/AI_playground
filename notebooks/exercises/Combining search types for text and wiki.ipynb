{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"../global_setup.py\") as setupfile:\n",
    "        exec(setupfile.read())\n",
    "except FileNotFoundError:\n",
    "    print('Setup already completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "from gensim.models.fasttext import FastText\n",
    "from scipy.spatial.distance import cdist\n",
    "import re\n",
    "# ESA relatedness package\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parsed documents.\n",
      "Loading preprocessed documents.\n",
      "Wikipedia loaded.\n"
     ]
    }
   ],
   "source": [
    "from src.text.document_retrieval.wikipedia import Wikipedia # Generic Wikipedia class\n",
    "wikipedia = Wikipedia(\n",
    "    language=\"Danish\",\n",
    "    cache_directory_url=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rsspedia:\n",
    "    def __init__(self, wikipedia: Wikipedia):\n",
    "        self.search_results = []\n",
    "        self.content = self.wikipedia_results = None\n",
    "        # Initialize wikipedia\n",
    "        self.wikipedia = wikipedia\n",
    "        # Remove all the line breaks and caret returns from wiki texts\n",
    "        pattern = re.compile('[\\n\\r ]+', re.UNICODE)\n",
    "        self.texts = [self.wikipedia.documents[i].text for i in range(len(self.wikipedia.documents))]\n",
    "        self.texts = [pattern.sub(' ', self.texts[i]) for i in range(len(self.texts))]\n",
    "        \n",
    "        # Calculate tf-idf representation for wiki texts (takes time)\n",
    "        self._transformer = TfidfVectorizer(stop_words = None, norm = \"l2\", use_idf = True, sublinear_tf = False)\n",
    "        self._Y = self._transformer.fit_transform(self.texts)\n",
    "        \n",
    "        # Fasttext: initialize the model\n",
    "        bin_path = Path(\"data\", \"fasttext\", \"wiki.da.bin\")\n",
    "        self.model = FastText.load_fasttext_format(str(bin_path))\n",
    "        \n",
    "        # Fasttext: Compute vectorized representation for all wikipedia articles (takes time)\n",
    "        i = 0\n",
    "        i_max = 0\n",
    "        n_removed = 0\n",
    "        self.wikipedia.documents_clean = self.wikipedia.documents.copy()\n",
    "        self.wikipedia_abstract_vectors = []\n",
    "        self.wikipedia_title_vectors = []\n",
    "        pattern1 = re.compile('[^a-zA-Z0-9åÅøØæÆ]+', re.UNICODE)\n",
    "\n",
    "        for n in range(len(self.wikipedia.documents)):\n",
    "            # if abstract length is zero, remove it\n",
    "            try:\n",
    "                if len(pattern1.sub('', self.wikipedia.documents[n].abstract)) == 0:\n",
    "                    del self.wikipedia.documents_clean[n - n_removed]\n",
    "                    n_removed = n_removed + 1\n",
    "                else:\n",
    "                    self.wikipedia_abstract_vectors.append(self.sumVectorRepresentation(text = self.wikipedia.documents[n].abstract))\n",
    "                    self.wikipedia_title_vectors.append(self.sumVectorRepresentation(text = self.wikipedia.documents[n].title))\n",
    "                    \n",
    "                    i = i + 1\n",
    "                    if i_max > 0 and i > i_max:\n",
    "                        break\n",
    "            except IndexError as e:\n",
    "                print(\"n: {}, n_removed: {}, w.d: {}, w.d_c: {}\".format(n, n_removed, len(self.wikipedia.documents), len(self.wikipedia.documents_clean)))\n",
    "  \n",
    "\n",
    "    def loadTexts(self, texts):\n",
    "        self.texts = texts\n",
    "    \n",
    "    def sumVectorRepresentation(self, text, verbose = False):\n",
    "        # Calculates vectorized represetnation of some text\n",
    "        pattern = re.compile('[^a-zA-Z0-9åÅøØæÆ ]+', re.UNICODE)\n",
    "        text = pattern.sub('', text)\n",
    "        words = text.lower().strip().split()\n",
    "        text_vector = np.zeros(self.model.wv[\"a\"].shape)\n",
    "        if verbose:\n",
    "            print(\"len: {}, words: {}\".format(len(words), words))\n",
    "        for i in range(len(words)):\n",
    "            try:\n",
    "                text_vector = text_vector + self.model.wv[words[i]]\n",
    "            except KeyError as e:\n",
    "                if verbose:\n",
    "                    print(\"i: {}, e: {}\".format(i, e))\n",
    "                continue\n",
    "        return text_vector\n",
    "    \n",
    "    def get_ngrams(self, text):\n",
    "        pattern = re.compile('[^a-zA-Z0-9åÅøØæÆ ]+', re.UNICODE)\n",
    "        s = pattern.sub('', text)\n",
    "        words = s.lower().strip().split()\n",
    "        words_copy = words.copy()\n",
    "        n_removed = 0\n",
    "        stop_words = [\"den\", \"det\", \"en\", \"et\", \"om\", \"for\", \"til\", \"at\", \"på\", \"jeg\", \"mig\", \"mine\", \"min\", \"mit\", \"du\", \"dig\", \"din\", \"dit\", \"dine\", \"han\", \"ham\", \"hun\", \"hende\", \"de\", \"dem\", \"vi\", \"os\", \"sin\", \"sit\", \"sine\", \"sig\"]\n",
    "        for i in range(len(words)):\n",
    "            if words[i] in stop_words:\n",
    "                words_copy.pop(i - n_removed)\n",
    "                n_removed = n_removed + 1\n",
    "\n",
    "        for i in range(len(words_copy)):\n",
    "            if i > 0:\n",
    "                words_copy.append(words_copy[i - 1] + \" \" + words_copy[i])\n",
    "\n",
    "        return words_copy\n",
    "    \n",
    "    def cdist_func(self, A, B):\n",
    "        # Calculates cosine distance\n",
    "        dists = cdist(A, B, 'cosine')\n",
    "        return np.argmin(dists, axis=0), dists #np.min(dists, axis=0)\n",
    "\n",
    "    def display_beautifully(self, titles, texts, urls):\n",
    "        formatted_result_list = [\"<ol>\"]\n",
    "        for i in range(len(titles)):\n",
    "            formatted_result = \"\\n\".join([\n",
    "                \"<li>\",\n",
    "                f\"<p><a href=\\\"{urls[i]}\\\">{titles[i]}</a></p>\",\n",
    "                f\"<p>{texts[i]}</p>\",\n",
    "                \"</li>\"\n",
    "            ])\n",
    "            formatted_result_list.append(formatted_result)\n",
    "        formatted_result_list.append(\"</ol>\")\n",
    "        formatted_results = \"\\n\".join(formatted_result_list)\n",
    "        return formatted_results\n",
    "\n",
    "    def search_wiki(self, search_texts, n_matches = 3, search_type = \"okapibm25\", verbose = False):\n",
    "        \n",
    "        titles = [] \n",
    "        texts = []\n",
    "        urls = []\n",
    "        \n",
    "        # (1) Remove unnecessary symbols from the search text\n",
    "        pattern = re.compile('[^a-zA-Z0-9åÅøØæÆ ]+', re.UNICODE)\n",
    "\n",
    "        if search_texts:\n",
    "            for i, text in enumerate(search_texts):\n",
    "                # (1) Remove unnecessary symbols from the search text\n",
    "                text = pattern.sub('', text)\n",
    "                \n",
    "                if search_type == \"okapibm25\":\n",
    "                    wikipedia_results, search_terms = self.wikipedia.search(query = text, k_1 = 1.2, b = 0.75)\n",
    "                    for index, score in wikipedia_results[:n_matches].items():\n",
    "                        document = self.wikipedia.documents[index]\n",
    "                        titles.append(document.title)\n",
    "                        texts.append(document.abstract)\n",
    "                        urls.append(document.url)\n",
    "                elif search_type == \"esa_relatedness\":\n",
    "                    y = self._transformer.transform([text])\n",
    "                    D = np.array((self._Y * y.T).todense())\n",
    "                    indices = np.argsort(-D, axis=0)\n",
    "                    titles = [self.wikipedia.documents[index].title for index in indices[:n_matches, 0]]\n",
    "                    texts = [self.wikipedia.documents[index].abstract for index in indices[:n_matches, 0]]\n",
    "                    urls = [self.wikipedia.documents[index].url for index in indices[:n_matches, 0]]\n",
    "                elif search_type == \"fasttext_a\":\n",
    "                    # Calculate the vectorized representation\n",
    "                    text_vector = self.sumVectorRepresentation(text)\n",
    "\n",
    "                    cdist_result = self.cdist_func(self.wikipedia_abstract_vectors, [text_vector])\n",
    "                    cdist_list = cdist_result[1] # List of all the cosine distances\n",
    "                    cdist_list_sorted = np.sort(cdist_list, axis = 0) # Sorted list of cosine distances - to get top N matches\n",
    "\n",
    "                    for i in range(n_matches):\n",
    "                        result = np.where(cdist_list == cdist_list_sorted[i])\n",
    "                        document = self.wikipedia.documents_clean[result[0][0]]\n",
    "                        titles.append(document.title)\n",
    "                        texts.append(document.abstract)\n",
    "                        urls.append(document.url)\n",
    "                elif search_type == \"fasttext_b\":\n",
    "                    ngrams = self.get_ngrams(text)\n",
    "                    r = []\n",
    "                    for i in range(len(ngrams)):\n",
    "                        cdist_result = self.cdist_func(self.wikipedia_title_vectors, [self.sumVectorRepresentation(text = ngrams[i])])\n",
    "                        cdist_result2 = self.cdist_func([self.sumVectorRepresentation(text = text)], [self.sumVectorRepresentation(text = ngrams[i])])\n",
    "\n",
    "                        cdist_list = cdist_result[1] # List of all the cosine distances\n",
    "                        cdist_list2 = cdist_result2[1]\n",
    "                        cdist_list = (cdist_list + cdist_list2) / 2\n",
    "                        cdist_list_sorted = np.sort(cdist_list, axis = 0) # Sorted list of cosine distances - to get top N matches\n",
    "                        r.append(np.where(cdist_list == cdist_list_sorted[0])[0])\n",
    "                        if verbose:\n",
    "                            print(\"n-gram: {}, dist: {}\".format(ngrams[i], cdist_list_sorted[0]))\n",
    "                    #d_list = []\n",
    "                    d_list = [(r[i][0], cdist_list[r[i][0]][0]) for i in range(len(r))]\n",
    "                    #for i in range(len(r)):\n",
    "                    #    d_list.append((r[i][0], cdist_list[r[i][0]][0]))\n",
    "                    \n",
    "                    d_list = sorted(d_list, key=lambda tup: tup[1])\n",
    "                    \n",
    "                    for i in range(n_matches):\n",
    "                        document = self.wikipedia.documents_clean[d_list[i][0]]\n",
    "                        titles.append(document.title)\n",
    "                        texts.append(document.abstract)\n",
    "                        urls.append(document.url)\n",
    "                \n",
    "        return titles, texts, urls\n",
    "\n",
    "rsspedia = Rsspedia(wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Den nøgne sandhed', 'Reputation management', 'De nøgne træer']\n",
      "['Sandhed', 'Søren', 'Jørgen Hansen (bokser)']\n",
      "['Race Against Time: Searching for Hope in AIDS-Ravaged Africa',\n",
      " 'Max Schmeling',\n",
      " 'Politisk ledelse']\n",
      "['USA', 'USA', 'Forhold']\n"
     ]
    }
   ],
   "source": [
    "test_headline = \"FN's verdensmål sættes i centrum på Folkemødet\"\n",
    "test_headline = \"Google fyrer 13 chefer og 35 medarbejdere for sexchikane\"\n",
    "test_headline = \"Søren Hansen om den »nøgne sandhed«: Erdogan går på diplomatiske listefødder for at forbedre sit forhold til USA\"\n",
    "\n",
    "titles, texts, urls = rsspedia.search_wiki(search_texts = [test_headline], search_type = \"okapibm25\")\n",
    "pprint.pprint(titles)\n",
    "\n",
    "titles, texts, urls = rsspedia.search_wiki(search_texts = [test_headline], search_type = \"esa_relatedness\")\n",
    "pprint.pprint(titles)\n",
    "\n",
    "titles, texts, urls = rsspedia.search_wiki(search_texts = [test_headline], search_type = \"fasttext_a\")\n",
    "pprint.pprint(titles)\n",
    "\n",
    "titles, texts, urls = rsspedia.search_wiki(search_texts = [test_headline], search_type = \"fasttext_b\", verbose = False)\n",
    "pprint.pprint(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('[\\n\\r ]+', re.UNICODE)\n",
    "texts = [wikipedia.documents[i].text for i in range(len(wikipedia.documents))]\n",
    "texts = [pattern.sub(' ', texts[i]) for i in range(len(texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern.sub(' ', texts[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
