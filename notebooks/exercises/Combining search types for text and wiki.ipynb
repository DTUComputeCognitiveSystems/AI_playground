{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"../global_setup.py\") as setupfile:\n",
    "        exec(setupfile.read())\n",
    "except FileNotFoundError:\n",
    "    print('Setup already completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "from gensim.models.fasttext import FastText\n",
    "from scipy.spatial.distance import cdist\n",
    "import re\n",
    "# ESA relatedness package\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parsed documents.\n",
      "Loading preprocessed documents.\n",
      "Wikipedia loaded.\n"
     ]
    }
   ],
   "source": [
    "from src.text.document_retrieval.wikipedia import Wikipedia # Generic Wikipedia class\n",
    "wikipedia = Wikipedia(\n",
    "    language=\"Danish\",\n",
    "    cache_directory_url=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "---------------------------------------------------------------------------",
      "NameError                                 Traceback (most recent call last)",
      "<ipython-input-66-38fb8984ff81> in <module>()\n     77         return text_vector\n     78     \n---> 79 rsspediainit = RsspediaInit(wikipedia)\n",
      "<ipython-input-66-38fb8984ff81> in __init__(self, wikipedia)\n     13         # Calculate tf-idf representation for wiki texts (takes time)\n     14         self._transformer = TfidfVectorizer(stop_words = None, norm = \"l2\", use_idf = True, sublinear_tf = False)\n---> 15         self._Y = self._transformer.fit_transform(texts_clean)\n     16         \n     17         # Fasttext: initialize the model\n",
      "NameError: name 'texts_clean' is not defined"
     ]
    }
   ],
   "source": [
    "class RsspediaInit:\n",
    "    def __init__(self, wikipedia: Wikipedia):\n",
    "        self.search_results = []\n",
    "        self.content = self.wikipedia_results = None\n",
    "        # Initialize wikipedia\n",
    "        self.wikipedia = wikipedia\n",
    "        # Remove all the line breaks and caret returns from wiki texts\n",
    "        pattern = re.compile('[\\n\\r ]+', re.UNICODE)\n",
    "        self.texts = [self.wikipedia.documents[i].text for i in range(len(self.wikipedia.documents))]\n",
    "        self.texts = [pattern.sub(' ', self.texts[i]) for i in range(len(self.texts))]\n",
    "        self.texts_clean = [self.getCleanWordsList(self.texts[i], return_string = True) for i in range(len(self.texts))]\n",
    "        \n",
    "        # Calculate tf-idf representation for wiki texts (takes time)\n",
    "        self._transformer = TfidfVectorizer(stop_words = None, norm = \"l2\", use_idf = True, sublinear_tf = False)\n",
    "        self._Y = self._transformer.fit_transform(texts_clean)\n",
    "        \n",
    "        # Fasttext: initialize the model\n",
    "        bin_path = Path(\"data\", \"fasttext\", \"wiki.da.bin\")\n",
    "        self.model = FastText.load_fasttext_format(str(bin_path))\n",
    "        \n",
    "        # Fasttext: Compute vectorized representation for all wikipedia articles (takes time)\n",
    "        i = 0\n",
    "        i_max = 0\n",
    "        n_removed = 0\n",
    "        self.wikipedia.documents_clean = self.wikipedia.documents.copy()\n",
    "        self.wikipedia_abstract_vectors = []\n",
    "        self.wikipedia_title_vectors = []\n",
    "        pattern1 = re.compile('[^a-zA-Z0-9åÅøØæÆ]+', re.UNICODE)\n",
    "\n",
    "        for n in range(len(self.wikipedia.documents)):\n",
    "            # if abstract length is zero, remove it\n",
    "            try:\n",
    "                if len(pattern1.sub('', self.wikipedia.documents[n].abstract)) == 0:\n",
    "                    del self.wikipedia.documents_clean[n - n_removed]\n",
    "                    n_removed = n_removed + 1\n",
    "                else:\n",
    "                    self.wikipedia_abstract_vectors.append(self.sumVectorRepresentation(text = self.wikipedia.documents[n].abstract))\n",
    "                    self.wikipedia_title_vectors.append(self.sumVectorRepresentation(text = self.wikipedia.documents[n].title))\n",
    "                    \n",
    "                    i = i + 1\n",
    "                    if i_max > 0 and i > i_max:\n",
    "                        break\n",
    "            except IndexError as e:\n",
    "                print(\"n: {}, n_removed: {}, w.d: {}, w.d_c: {}\".format(n, n_removed, len(self.wikipedia.documents), len(self.wikipedia.documents_clean)))\n",
    "    \n",
    "    def getCleanWordsList(self, text, return_string = False):\n",
    "        pattern = re.compile('[^a-zA-Z0-9åÅøØæÆ ]+', re.UNICODE)\n",
    "        text = pattern.sub('', text)\n",
    "        words = text.lower().strip().split()\n",
    "        words_copy = words.copy()\n",
    "        stop_words = [\"den\", \"det\", \"en\", \"et\", \"om\", \"for\", \"til\", \"at\", \"af\", \"på\", \"som\", \"og\", \n",
    "                      \"jeg\", \"mig\", \"mine\", \"min\", \"mit\", \"du\", \"dig\", \"din\", \"dit\", \"dine\", \"han\", \"ham\", \"hun\", \"hende\", \n",
    "                      \"de\", \"dem\", \"vi\", \"os\", \"sin\", \"sit\", \"sine\", \"sig\"]\n",
    "        n_removed = 0\n",
    "        for i in range(len(words)):\n",
    "            if words[i] in stop_words:\n",
    "                words_copy.pop(i - n_removed)\n",
    "                n_removed = n_removed + 1\n",
    "        if return_string:\n",
    "            return ' '.join(words_copy)\n",
    "        else:\n",
    "            return words_copy\n",
    "    \n",
    "    def sumVectorRepresentation(self, text, verbose = False):\n",
    "        # Calculates vectorized represetnation of some text\n",
    "        words = self.getCleanWordsList(text)\n",
    "        text_vector = np.zeros(self.model.wv[\"a\"].shape)\n",
    "        if verbose:\n",
    "            print(\"len: {}, words: {}\".format(len(words), words))\n",
    "        for i in range(len(words)):\n",
    "            try:\n",
    "                text_vector = text_vector + self.model.wv[words[i]]\n",
    "            except KeyError as e:\n",
    "                if verbose:\n",
    "                    print(\"i: {}, e: {}\".format(i, e))\n",
    "                continue\n",
    "        return text_vector\n",
    "    \n",
    "rsspediainit = RsspediaInit(wikipedia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rsspedia:\n",
    "    def __init__(self, wikipedia: Wikipedia, rsspediainit: RsspediaInit):\n",
    "        self.search_results = []\n",
    "        self.content = self.wikipedia_results = None\n",
    "        # Initialize wikipedia\n",
    "        self.wikipedia = wikipedia\n",
    "        \n",
    "        self.texts = rsspediainit.texts\n",
    "        \n",
    "        # Calculate tf-idf representation for wiki texts (takes time)\n",
    "        self._transformer = rsspediainit._transformer\n",
    "        self._Y = rsspediainit._Y\n",
    "        \n",
    "        # Fasttext: initialize the model\n",
    "        self.model = rsspediainit.model\n",
    "        \n",
    "        #self.wikipedia.documents_clean \n",
    "        self.wikipedia_abstract_vectors = rsspediainit.wikipedia_abstract_vectors\n",
    "        self.wikipedia_title_vectors = rsspediainit.wikipedia_title_vectors\n",
    "    \n",
    "    def getCleanWordsList(self, text, return_string = False):\n",
    "        pattern = re.compile('[^a-zA-Z0-9åÅøØæÆ ]+', re.UNICODE)\n",
    "        text = pattern.sub('', text)\n",
    "        words = text.lower().strip().split()\n",
    "        words_copy = words.copy()\n",
    "        stop_words = [\"den\", \"det\", \"en\", \"et\", \"om\", \"for\", \"til\", \"at\", \"af\", \"på\", \"som\", \"og\", \n",
    "                      \"jeg\", \"mig\", \"mine\", \"min\", \"mit\", \"du\", \"dig\", \"din\", \"dit\", \"dine\", \"han\", \"ham\", \"hun\", \"hende\", \n",
    "                      \"de\", \"dem\", \"vi\", \"os\", \"sin\", \"sit\", \"sine\", \"sig\"]\n",
    "        n_removed = 0\n",
    "        for i in range(len(words)):\n",
    "            if words[i] in stop_words:\n",
    "                words_copy.pop(i - n_removed)\n",
    "                n_removed = n_removed + 1\n",
    "        if return_string:\n",
    "            return ' '.join(words_copy)\n",
    "        else:\n",
    "            return words_copy\n",
    "        \n",
    "    def sumVectorRepresentation(self, text, verbose = False):\n",
    "        # Calculates vectorized represetnation of some text\n",
    "        words = self.getCleanWordsList(text)\n",
    "        text_vector = np.zeros(self.model.wv[\"a\"].shape)\n",
    "        if verbose:\n",
    "            print(\"len: {}, words: {}\".format(len(words), words))\n",
    "        for i in range(len(words)):\n",
    "            try:\n",
    "                text_vector = text_vector + self.model.wv[words[i]]\n",
    "            except KeyError as e:\n",
    "                if verbose:\n",
    "                    print(\"i: {}, e: {}\".format(i, e))\n",
    "                continue\n",
    "        return text_vector\n",
    "    \n",
    "    def get_ngrams(self, text):\n",
    "        words = self.getCleanWordsList(text)\n",
    "        words_copy = words.copy()\n",
    "        n_removed = 0\n",
    "        \n",
    "\n",
    "        for i in range(len(words_copy)):\n",
    "            if i > 0:\n",
    "                words_copy.append(words_copy[i - 1] + \" \" + words_copy[i])\n",
    "\n",
    "        return words_copy\n",
    "    \n",
    "    def cdist_func(self, A, B):\n",
    "        # Calculates cosine distance\n",
    "        dists = cdist(A, B, 'cosine')\n",
    "        return np.argmin(dists, axis=0), dists #np.min(dists, axis=0)\n",
    "\n",
    "    def display_beautifully(self, titles, texts, urls):\n",
    "        formatted_result_list = [\"<ol>\"]\n",
    "        for i in range(len(titles)):\n",
    "            formatted_result = \"\\n\".join([\n",
    "                \"<li>\",\n",
    "                f\"<p><a href=\\\"{urls[i]}\\\">{titles[i]}</a></p>\",\n",
    "                f\"<p>{texts[i]}</p>\",\n",
    "                \"</li>\"\n",
    "            ])\n",
    "            formatted_result_list.append(formatted_result)\n",
    "        formatted_result_list.append(\"</ol>\")\n",
    "        formatted_results = \"\\n\".join(formatted_result_list)\n",
    "        return formatted_results\n",
    "\n",
    "    def search_wiki(self, search_texts, n_matches = 3, search_type = \"okapibm25\", verbose = False, p = 0.6):\n",
    "        \n",
    "        titles = [] \n",
    "        texts = []\n",
    "        urls = []\n",
    "        \n",
    "        # (1) Remove unnecessary symbols from the search text\n",
    "        pattern = re.compile('[^a-zA-Z0-9åÅøØæÆ ]+', re.UNICODE)\n",
    "\n",
    "        if search_texts:\n",
    "            for i, text in enumerate(search_texts):\n",
    "                # (1) Remove unnecessary symbols from the search text\n",
    "                text = pattern.sub('', text)\n",
    "                \n",
    "                if search_type == \"okapibm25\":\n",
    "                    wikipedia_results, search_terms = self.wikipedia.search(query = text, k_1 = 1.2, b = 0.75)\n",
    "                    for index, score in wikipedia_results[:n_matches].items():\n",
    "                        document = self.wikipedia.documents[index]\n",
    "                        titles.append(document.title)\n",
    "                        texts.append(document.abstract)\n",
    "                        urls.append(document.url)\n",
    "                elif search_type == \"esa_relatedness\":\n",
    "                    y = self._transformer.transform([text])\n",
    "                    D = np.array((self._Y * y.T).todense())\n",
    "                    indices = np.argsort(-D, axis=0)\n",
    "                    titles = [self.wikipedia.documents[index].title for index in indices[:n_matches, 0]]\n",
    "                    texts = [self.wikipedia.documents[index].abstract for index in indices[:n_matches, 0]]\n",
    "                    urls = [self.wikipedia.documents[index].url for index in indices[:n_matches, 0]]\n",
    "                elif search_type == \"fasttext_a\":\n",
    "                    # Calculate the vectorized representation\n",
    "                    text_vector = self.sumVectorRepresentation(text)\n",
    "\n",
    "                    cdist_result = self.cdist_func(self.wikipedia_abstract_vectors, [text_vector])\n",
    "                    cdist_list = cdist_result[1] # List of all the cosine distances\n",
    "                    cdist_list_sorted = np.sort(cdist_list, axis = 0) # Sorted list of cosine distances - to get top N matches\n",
    "\n",
    "                    for i in range(n_matches):\n",
    "                        result = np.where(cdist_list == cdist_list_sorted[i])\n",
    "                        document = self.wikipedia.documents_clean[result[0][0]]\n",
    "                        titles.append(document.title)\n",
    "                        texts.append(document.abstract)\n",
    "                        urls.append(document.url)\n",
    "                elif search_type == \"fasttext_b\":\n",
    "                    ngrams = self.get_ngrams(text)\n",
    "                    r = []\n",
    "                    for i in range(len(ngrams)):\n",
    "                        cdist_result = self.cdist_func(self.wikipedia_title_vectors, [self.sumVectorRepresentation(ngrams[i])])\n",
    "                        cdist_result2 = self.cdist_func([self.sumVectorRepresentation(text)], [self.sumVectorRepresentation(ngrams[i])])\n",
    "\n",
    "                        cdist_list1 = cdist_result[1] # List of all the cosine distances\n",
    "                        cdist_list2 = cdist_result2[1]\n",
    "                        cdist_list = (cdist_list1 * p + cdist_list2 * (1 - p))\n",
    "                        cdist_list_sorted = np.sort(cdist_list, axis = 0) # Sorted list of cosine distances - to get top N matches\n",
    "                        \n",
    "                        for j in range(5):\n",
    "                            x = np.where(cdist_list == cdist_list_sorted[j])[0]\n",
    "                            r.append( (x, cdist_list[x][0]))\n",
    "                            if verbose:\n",
    "                                print(\"{} {} {} {}\".format(x, wikipedia.documents_clean[x[0]].title, cdist_list[x], ngrams[i]))\n",
    "\n",
    "                    # When np.where returns multiple matches, we flatten them\n",
    "                    r_copy = r.copy()\n",
    "                    uniques = []\n",
    "                    for i in range(len(r)-1, -1, -1):\n",
    "                        if len(r[i][0]) > 1:\n",
    "                            r_copy.pop(i)\n",
    "                            for j in range(len(r[i][0])):\n",
    "                                r_copy.append( (np.array([r[i][0][j]]), r[i][1]))\n",
    "\n",
    "                    # Remove duplicate wikipedia pages. They occur because different n-grams can match the same pages\n",
    "                    for i in range(len(r_copy)-1,-1,-1):\n",
    "                        if r_copy[i][0] in uniques:\n",
    "                            r_copy.pop(i)\n",
    "                        else:\n",
    "                            uniques.append(r_copy[i][0])\n",
    "                    \n",
    "                    r = r_copy\n",
    "                    # Transform into list of tuples\n",
    "                    r = [ (r[i][0][0], r[i][1][0]) for i in range(len(r))]\n",
    "                    # Sort the list of tuples by cosine distance\n",
    "                    r = sorted(r, key=lambda tup: tup[1])\n",
    "                    \n",
    "                    for i in range(n_matches):\n",
    "                        document = self.wikipedia.documents_clean[r[i][0]]\n",
    "                        titles.append(document.title)\n",
    "                        texts.append(document.abstract)\n",
    "                        urls.append(document.url)\n",
    "                \n",
    "        return titles, texts, urls\n",
    "\n",
    "\n",
    "rsspedia = Rsspedia(wikipedia, rsspediainit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beredskabsstyrelsen Højskole',\n",
      " 'Sexchikane',\n",
      " 'Beredskabsstyrelsen Teknisk Skole']\n",
      "['Google Apps for Work', 'Google', 'Google Briller']\n",
      "['Watch Medier', 'Bladkompagniet', 'Associated Press']\n",
      "['Google', 'Google+', 'Sexchikane']\n"
     ]
    }
   ],
   "source": [
    "#test_headline = \"FN's verdensmål sættes i centrum på Folkemødet\"\n",
    "test_headline = \"Google fyrer 13 chefer og 35 medarbejdere for sexchikane\"\n",
    "#test_headline = \"Søren Hansen om den »nøgne sandhed«: Erdogan går på diplomatiske listefødder for at forbedre sit forhold til USA\"\n",
    "\n",
    "titles, texts, urls = rsspedia.search_wiki(search_texts = [test_headline], search_type = \"okapibm25\")\n",
    "pprint.pprint(titles)\n",
    "\n",
    "titles, texts, urls = rsspedia.search_wiki(search_texts = [test_headline], search_type = \"esa_relatedness\")\n",
    "pprint.pprint(titles)\n",
    "\n",
    "titles, texts, urls = rsspedia.search_wiki(search_texts = [test_headline], search_type = \"fasttext_a\")\n",
    "pprint.pprint(titles)\n",
    "\n",
    "titles, texts, urls = rsspedia.search_wiki(search_texts = [test_headline], search_type = \"fasttext_b\")\n",
    "pprint.pprint(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sexchikane',\n",
      " 'Google',\n",
      " 'Google+',\n",
      " 'Google Hangouts',\n",
      " 'Google (søgemaskine)',\n",
      " 'Google Briller',\n",
      " 'Medarbejder',\n",
      " 'Servicemedarbejder',\n",
      " 'Chikane',\n",
      " 'Medarbejdende ægtefælle',\n",
      " 'Når udsatte bliver ansatte',\n",
      " 'Google AdSense',\n",
      " '13',\n",
      " '1340',\n",
      " 'Fremmedarbejderbladet',\n",
      " 'Den 13. forfatningsændring',\n",
      " 'Medarbejderinvesteringsselskab',\n",
      " '35',\n",
      " '13. januar',\n",
      " '1388']\n"
     ]
    }
   ],
   "source": [
    "titles, texts, urls = rsspedia.search_wiki(search_texts = [test_headline], n_matches = 20, search_type = \"fasttext_b\", verbose = False, p = 0.5)\n",
    "pprint.pprint(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
