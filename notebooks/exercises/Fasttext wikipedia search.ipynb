{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Fasttext word embeddings to search Danish Wikipedia\n",
    "This notebook goes step-by-step through the following:\n",
    "1. Load the Fasttext model for Danish language\n",
    "2. Write a function to compute the vectorized representations of text in Danish\n",
    "3. Compute vectorized representations for all the abstracts in Danish wikipedia\n",
    "4. Write a function to compute the cosine distance between the vectorized representations\n",
    "5. Test everything on some news headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"../global_setup.py\") as setupfile:\n",
    "        exec(setupfile.read())\n",
    "except FileNotFoundError:\n",
    "    print('Setup already completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from gensim.test.utils import common_texts\n",
    "from scipy.spatial.distance import cdist\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Fasttext model for Danish language\n",
    "1. In the root folder of the project, create a directory \"data\" and a subdirectory \"fasttext\".\n",
    "2. Download the pretrained danish fasttext model from here: \n",
    "   https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.da.zip\n",
    "3. Place the files **wiki.da.bin** and **wiki.da.vec** in the folder you have created in (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_path = Path(\"data\", \"fasttext\", \"wiki.da.bin\")\n",
    "model = FastText.load_fasttext_format(str(bin_path))\n",
    "#print(model.most_similar('æble'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "1. Compute vectorized representations of all danish wikipedia abstracts\n",
    "2. Compute vectorized representations of news titles\n",
    "3. Find wikipedia article with minimal cosine distance from a given news title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Working with Danish wikipedia abstracts\n",
    "Here we compute the vectorized representations of the Danish wikipedia abstracts using the pretrained Fasttext model that we just loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step: load the Danish wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.text.document_retrieval.wikipedia import Wikipedia\n",
    "wikipedia = Wikipedia(\n",
    "    language=\"Danish\",\n",
    "    cache_directory_url=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second step: write a function to calculate the vectorized representation of any text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumVectorRepresentation(text, verbose = False):\n",
    "    pattern = re.compile('[^a-zA-Z0-9åÅøØæÆ ]+', re.UNICODE)\n",
    "    text = pattern.sub('', text)\n",
    "    words = text.lower().strip().split()\n",
    "    text_vector = np.zeros(model.wv[\"a\"].shape)\n",
    "    if verbose:\n",
    "        print(\"len: {}, words: {}\".format(len(words), words))\n",
    "    for i in range(len(words)):\n",
    "        try:\n",
    "            text_vector = text_vector + model.wv[words[i]]\n",
    "        except KeyError as e:\n",
    "            if verbose:\n",
    "                print(\"i: {}, e: {}\".format(i, e))\n",
    "            continue\n",
    "    return text_vector\n",
    "    \n",
    "#sumVectorRepresentation(\"Han sagde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third step: compute vectorized representations for all Danish wikipedia abstracts\n",
    "* All empty abstracts and those that do not have any alphanumeric symbols are removed and not considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "i_max = 0\n",
    "n_removed = 0\n",
    "wikipedia.documents_clean = wikipedia.documents.copy()\n",
    "wikipedia_abstract_vectors = []\n",
    "pattern1 = re.compile('[^a-zA-Z0-9åÅøØæÆ]+', re.UNICODE)\n",
    "\n",
    "for n in range(len(wikipedia.documents)):\n",
    "    # if abstract length is zero, remove it\n",
    "    try:\n",
    "        if len(pattern1.sub('', wikipedia.documents[n].abstract)) == 0:\n",
    "            del wikipedia.documents_clean[n - n_removed]\n",
    "            n_removed = n_removed + 1\n",
    "        else:\n",
    "            wikipedia_abstract_vectors.append(sumVectorRepresentation(wikipedia.documents[n].abstract))\n",
    "            i = i + 1\n",
    "            if i_max > 0 and i > i_max:\n",
    "                break\n",
    "    except IndexError as e:\n",
    "        print(\"n: {}, n_removed: {}, w.d: {}, w.d_c: {}\".format(n, n_removed, len(wikipedia.documents), len(wikipedia.documents_clean)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate the cosine distance between the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdist_func(A, B):\n",
    "    dists = cdist(A, B, 'cosine')\n",
    "    return np.argmin(dists, axis=0), dists #np.min(dists, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test the results on some news headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable below defines the number of relevant wikipedia articles to consider\n",
    "n_wiki_matches = 3\n",
    "# Variable below is an example news headline\n",
    "example_title = \"Hawaii: Flyv med helikopter hen over Kauai — en af verdens smukkeste øer\"\n",
    "# Hawaii: Flyv med helikopter hen over Kauai — en af verdens smukkeste øer\n",
    "# Tyske myndigheder undersøger 95.000 biler af mærket Opel\n",
    "# Salmonella fundet i kalkunbryst solgt i Aldi-butikker\n",
    "\n",
    "\n",
    "# Calculate the vectorized representation\n",
    "example_title_vector = sumVectorRepresentation(example_title)\n",
    "\n",
    "cdist_result = cdist_func(wikipedia_abstract_vectors, [example_title_vector])\n",
    "cdist_list = cdist_result[1] # List of all the cosine distances\n",
    "cdist_list_sorted = np.sort(cdist_list, axis = 0) # Sorted list of cosine distances - to get top N matches\n",
    "\n",
    "# Print the results\n",
    "print(\"Example headline: {}\\r\\n\".format(example_title))\n",
    "## Print all the matches with their abstracts\n",
    "for i in range(n_wiki_matches):\n",
    "    result = np.where(cdist_list == cdist_list_sorted[i])\n",
    "    print(\"{} Wikipedia article {}: \\r\\n Abstract: {}\\r\\n\".format(i, \n",
    "                                                       wikipedia.documents_clean[result[0][0]],\n",
    "                                                       wikipedia.documents_clean[result[0][0]].abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from ipywidgets.widgets import Accordion, HTML\n",
    "from notebooks.exercises.src.text.news_sentiment_1 import RSSDashboard\n",
    "RSSdb = RSSDashboard()\n",
    "s = RSSdb._do_sentiment_analysis(selected_value = 0)\n",
    "\n",
    "list_labels = []\n",
    "for i in range(len(RSSdb.data_titles)):\n",
    "    result_content = \"<ol>\"\n",
    "    cdist_result = cdist_func(wikipedia_abstract_vectors, [sumVectorRepresentation(RSSdb.data_titles[i])])\n",
    "    cdist_list = cdist_result[1] # List of all the cosine distances\n",
    "    cdist_list_sorted = np.sort(cdist_list, axis = 0) # Sorted list of cosine distances - to get top N matches\n",
    "\n",
    "    ## Print all the matches with their abstracts\n",
    "    for i in range(n_wiki_matches):\n",
    "        result = np.where(cdist_list == cdist_list_sorted[i])\n",
    "        result_content = result_content + \"<li>{}: <p> {}</p>\".format(wikipedia.documents_clean[result[0][0]].title,\n",
    "                                                                          wikipedia.documents_clean[result[0][0]].abstract)\n",
    "    result_content = result_content + \"<ol>\"\n",
    "    list_labels.append(HTML(value = result_content))\n",
    "\n",
    "accordion = Accordion(children = (list_labels),)\n",
    "\n",
    "for i in range(len(RSSdb.data_titles)):\n",
    "    accordion.set_title(i, \"{}. {}\".format(i + 1, RSSdb.data_titles[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(accordion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
