{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSS Wikipedia Search Dashboard\n",
    "\n",
    "This notebook is a final product of work on two other notebooks:\n",
    "\n",
    "1. Dev - 1 - Fasttext wikipedia search\n",
    "2. Dev - 2 - Combining search types for text and wiki\n",
    "\n",
    "These 2 notebooks are intermediate development notebooks, final production content is in this notebook.\n",
    "\n",
    "Here, you will find a detailed description of the 4 search algorithms we use to find relevant Wikipedia articles for the newsfeed titles.\n",
    "\n",
    "The high-level contents of the notebook are following: \n",
    "\n",
    "1. Initialize Danish Wikipedia\n",
    "2. Initialize RsspediaInit class\n",
    "3. Initialize the RSSWikiDashboard class\n",
    "\n",
    "Under each heading, you will find a detailed description of the processes and algorithms involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"../global_setup.py\") as setupfile:\n",
    "        exec(setupfile.read())\n",
    "except FileNotFoundError:\n",
    "    print('Setup already completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_wrapper, .output {\n",
    "    height:auto !important;\n",
    "    max-height: 10000px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.text.document_retrieval.wikipedia import Wikipedia\n",
    "from notebooks.exercises.src.text.news_wiki_search_init import RsspediaInit\n",
    "from notebooks.exercises.src.text.news_wiki_dashboard import RSSWikiDashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Danish Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia = Wikipedia(\n",
    "    language=\"Danish\",\n",
    "    cache_directory_url=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize RsspediaInit class\n",
    "\n",
    "In this initialization, the data that is needed to perform search is loaded.\n",
    "1. Okapi BM-25: uses precomputed wikipedia tf and idf vectors. No more preprocessing is done.\n",
    "2. Explicit Semantic Analysis: here we load the stored tf-idf vectors or compute them and store on disk.\n",
    "3. FTN-a and FTN-b: here we load the stored Fasttext vectors for wikipedia titles and abstracts or compute them and store on disk. As a preprocessing, non-alphanumeric characters are removed, and zero-length abstracts are removed as well. Wikipedia documents and titles are adjusted accordingly.\n",
    "<br><br>\n",
    "For both (2) and (3) we remove the following stop-words: <br>\n",
    "<code>stop_words = [\"den\", \"det\", \"denne\", \"dette\", \"en\", \"et\", \"om\", \"for\", \"til\", \"at\", \"af\", \"på\", \"som\", \"og\", \"er\", \"i\"]</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_search_init = RsspediaInit(wikipedia = wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize and start the RSSWikiDashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Types of search and their descriptions\n",
    "\n",
    "Before searching the wikipedia, both wikipedia page texts and searchable texts are preprocessed:\n",
    "* Remove stopwords\n",
    "* Remove non-alphanumeric characters\n",
    "\n",
    "<code>self.texts_clean = [self.getCleanWordsList(self.texts[i], return_string = True) for i in range(len(self.texts))]</code><br>\n",
    "\n",
    "#### 3.1.1 Okapi BM-25\n",
    "\n",
    "In information retrieval, Okapi BM25 (BM stands for Best Matching) is a ranking function used by search engines to rank matching documents according to their relevance to a given search query. It is based on the probabilistic retrieval framework developed in the 1970s and 1980s by Stephen E. Robertson, Karen Spärck Jones, and others.\n",
    "\n",
    "The name of the actual ranking function is BM25. To set the right context, however, it is usually referred to as \"Okapi BM25\", since the Okapi information retrieval system, implemented at London's City University in the 1980s and 1990s, was the first system to implement this function.\n",
    "\n",
    "BM25 and its newer variants, e.g. BM25F (a version of BM25 that can take document structure and anchor text into account), represent state-of-the-art TF-IDF-like retrieval functions used in document retrieval.\n",
    "\n",
    "Given a query $Q$, containing keywords $ q_1,...,q_n$, the BM25 score of a document $D$ is:\n",
    "\n",
    "$$\n",
    "score(D,Q)=\\sum_{i=1}^n IDF(q_i) * \\frac{f(q_i,D)*(k_1+1)} {f(q_i,D)+k_1*(1-b+b*\\frac{|D|}{(avgdl)}},\n",
    "$$\n",
    "\n",
    "where $f(q_i,D)$ is $q_{i}$'s term frequency in the document $D$, $|D|$ is the length of the document $D$ in words, and $avgdl$ is the average document length in the text collection from which documents are drawn. $k_1$ and $b$ are free parameters, usually chosen, in absence of an advanced optimization, as $k_1\\in [1.2,2.0]$ and $b=0.75$. $IDF(q_i)$ is the IDF (inverse document frequency) weight of the query term $q_i$. \n",
    "\n",
    "It is usually computed as:\n",
    "\n",
    "$$\n",
    "IDF(q_i)=log((N-n(q_i)+0.5)/(n(q_i)+0.5),\n",
    "$$\n",
    "\n",
    "where $N$ is the total number of documents in the collection, and $n(q_i)$ is the number of documents containing $q_i$.\n",
    "\n",
    "\n",
    "#### 3.1.2 Explicit Semantic Analysis\n",
    "\n",
    "**First, we preprocess the texts.**\n",
    "* Remove line breaks (symbols \\r\\n) and replace them with whitespaces\n",
    "\n",
    "<code>pattern = re.compile('[\\n\\r ]+', re.UNICODE)</code><br>\n",
    "<code>self.texts = [pattern.sub(' ', self.texts[i]) for i in range(len(self.texts))]</code><br>\n",
    "\n",
    "**Second, we compute the TF-IDF representation of Danish Wikipedia article full texts.**\n",
    "* Initialize the TfidfVectorizer with L2 normalization\n",
    "* Transform the preprocessed texts into TF-IDF representations\n",
    "\n",
    "<code>self._transformer = TfidfVectorizer(stop_words = None, norm = \"l2\", use_idf = True, sublinear_tf = False)</code><br>\n",
    "<code>self._Y = self._transformer.fit_transform(self.texts_clean)</code>\n",
    "\n",
    "**Third, we compute the TF-IDF representation of the searchable text.**\n",
    "\n",
    "<code>y = self._transformer.transform([text])</code><br>\n",
    "\n",
    "**Fourth, we multiply the two sparce matrices and convert the result to a dense matrix (as np.array).**\n",
    "\n",
    "<code>D = np.array((self._Y * y.T).todense())</code>\n",
    "\n",
    "**Finally, we sort the results and get the indices of our matches.**\n",
    "\n",
    "<code>indices = np.argsort(-D, axis=0)</code>\n",
    "\n",
    "\n",
    "#### 3.1.3 FTN-a\n",
    "\n",
    "**First, we compute the vectorized representation of the Wikipedia title texts and the searchable text by summing up the representations of individual words**\n",
    "\n",
    "<code>text_vector = text_vector + self.model.wv[words[i]]</code><br>\n",
    "\n",
    "**Second, we compute the cosine distance between the searchable text vector and all the Wikipedia title vectors.**\n",
    "\n",
    "<code>cdist_list = cdist(wiki_title_vectors, searchable_text_vector, 'cosine')</code><br>\n",
    "\n",
    "**Third, we sort the list of distances and get the results**\n",
    "\n",
    "<code>cdist_list_sorted = np.sort(cdist_list, axis = 0)</code><br>\n",
    "<code>result = np.where(cdist_list == cdist_list_sorted[i])</code><br>\n",
    "\n",
    "\n",
    "#### 3.1.4 FTN-b\n",
    "\n",
    "\n",
    "**First, we compute the vectorized representation of the Wikipedia title texts.**\n",
    "\n",
    "<code>text_vector = text_vector + self.model.wv[words[i]]</code><br>\n",
    "\n",
    "**Second, we extract the n-grams (1 and 2 words) from the searchable text.**\n",
    "\n",
    "<code>ngrams = self.get_ngrams(text)</code><br>\n",
    "\n",
    "**For each n-gram, we compute the vectorized representation of that n-gram by summing up the vectors of individual words.**\n",
    "\n",
    "<code>text_vector = text_vector + self.model.wv[words[i]]</code><br>\n",
    "\n",
    "**Then, we compute the cosine distance between the searchable n-gram vector and all the Wikipedia title vectors and between the n-gram vector and the searchable text and combine the two using a parameter $p$.**\n",
    "\n",
    "This is done in order for n-grams to have lower cosine distance to the Wikipedia title, because the n-gram will have a lower cosine distance with the whole searchable text than a part of the n-gram.\n",
    "\n",
    "<code>cdist_list1 =  cdist(wiki_title_vectors, n_gram_vector, 'cosine')</code><br>\n",
    "<code>cdist_list2 =  cdist(searchable_text_vector, n_gram_vector, 'cosine')</code><br>\n",
    "<code>cdist_list = (cdist_list1 * p + cdist_list2 * (1 - p))</code><br>\n",
    "\n",
    "**Last, we sort the list of distances and get the results**\n",
    "\n",
    "<code>cdist_list_sorted = np.sort(cdist_list, axis = 0)</code><br>\n",
    "<code>result = np.where(cdist_list == cdist_list_sorted[i])</code><br>\n",
    "\n",
    "\n",
    "### 3.2 Post-processing\n",
    "\n",
    "Here, there is an option to exclude too similar results. Some wikipedia pages are similar and refer to similar entities. \n",
    "Cosine distance is measured between all the pairs of results, and if the result similarity is higher than some threshold, then the result is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rsswdb = RSSWikiDashboard(wikipedia, rss_search_init)\n",
    "rsswdb.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r = rsswdb.rsspedia_search.cdist_func([rss_search_init.sumVectorRepresentation(\"6\")],\n",
    "#    [rss_search_init.sumVectorRepresentation(\"Guide Anmelderne anbefaler 6 gode spisesteder med pasta på menuen\")])\n",
    "#import pprint\n",
    "#pprint.pprint(titles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
