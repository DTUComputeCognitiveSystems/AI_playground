{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run global setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"../global_setup.py\") as setupfile:\n",
    "        exec(setupfile.read())\n",
    "except FileNotFoundError:\n",
    "    print('Setup already completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run local setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.rl.CliffworldEnv import CliffworldEnv\n",
    "\n",
    "from src.rl.RandomAgent import RandomAgent\n",
    "from src.rl.util import run_episode_ss\n",
    "\n",
    "from src.rl.TabularQAgent import TabularQAgent\n",
    "from src.rl.util import run_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment, Agents, Actions and Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simplified way to observe the world would be an environment with some well-defined states wherein some agents act according to their goals. The states are defined by every unique situation which could possibly exist in the environment. The state of the environment changes as the agents perform actions, e.g. the agent is hungry, so it decides to eat and thereby the state of the refrigerator has changed to be less full.\n",
    "\n",
    "The most accessible digital version of such simplified worlds exist in form of computer games where the agent is the player or even non-player, i.e. computer, character attempting to take the best actions in order to reach the goal of the game.\n",
    "\n",
    "It is much harder to define the environment, agents, actions and goals of real-world scenarios in order to make it feasible to solve due to the complexities and many possible states of the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cliff World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize': (15, 15)})\n",
    "states_colors = matplotlib.colors.ListedColormap(\n",
    "    ['#9A9A9A', '#D886BA', '#4D314A', '#6E9183'])\n",
    "cmap_default = 'Blues'\n",
    "cpal_default = sns.color_palette((\"Blues_d\"))\n",
    "\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "env = CliffworldEnv()\n",
    "env.render(mode='reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RandomAgent()\n",
    "sum_r, ss = run_episode_ss(env, agent)\n",
    "\n",
    "env.render(mode='path', ss=ss)\n",
    "print(\"Reward achieved: \", sum_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env, agent, epsilon_decay, n_episodes) -> list:\n",
    "    rewards = []\n",
    "    for i in range(n_episodes):\n",
    "        sum_r = run_episode(env, agent, learn=True)\n",
    "        rewards.append(sum_r)\n",
    "        agent.epsilon *= epsilon_decay\n",
    "    agent.epsilon = 0\n",
    "    sum_r, ss = run_episode_ss(env, agent)\n",
    "    print('Trained for ', n_episodes, ' episodes. Last episode achieved a reward of ', sum_r, '. Last episode run: ')     \n",
    "    env.render(mode='path', ss=ss)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "alpha = 0.1  # learning rate\n",
    "epsilon = 1.0  # initial randomness\n",
    "gamma = 1.0  # discount factor\n",
    "agent = TabularQAgent(alpha, epsilon, gamma)\n",
    "\n",
    "rewards = run_experiment(env, agent, 0.99, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = sns.tsplot(rewards)\n",
    "figure.set(xlabel='# episodes', ylabel='reward')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(mode='policy', Q=agent.Q, A=agent.A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
