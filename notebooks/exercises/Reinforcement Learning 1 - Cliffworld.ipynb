{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run global setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"../global_setup.py\") as setupfile:\n",
    "        exec(setupfile.read())\n",
    "except FileNotFoundError:\n",
    "    print('Setup already completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run local setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.utility.widgets import TextAreaSubmit, SliderParameters\n",
    "\n",
    "from src.rl.CliffworldEnv import CliffworldEnv\n",
    "\n",
    "from src.rl.RandomAgent import RandomAgent\n",
    "from src.rl.util import run_episode_ss\n",
    "\n",
    "from src.rl.TabularQAgent import TabularQAgent\n",
    "from src.rl.util import run_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is reinforcement learning?\n",
    "Reinforcement learning distinguishes itself from supervised or unsupervised learning, mainly in that it aims to solve *sequential decision making* problems. This is a more active framing of the learning problem, and it consists of some fundamental concepts. In reinforcement learning, there is an *agent* that *observes* an *environment*, taking *actions* and receiving *rewards* for doing well. The agent should thus maximize the cumulative reward that it will receive. Centrally in reinforcement learning, we have the *reward hypothesis* [1]:\n",
    "\n",
    ">That all of what we mean by goals and purposes can be well thought of as\n",
    "the maximization of the expected value of the cumulative sum of a received\n",
    "scalar signal (called reward).\n",
    "\n",
    "Think about this for a bit. Do you agree? Whether you agree or not, this is the basis of reinforcement learning, and certainly a lot of useful goals can be described in this manner. However, it can also be very difficult to describe exactly the goal in your mind using the reward formalism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to reinforcement learning\n",
    "We're going to be somewhat formal in introducing some common terms and their meanings. However, these notebooks are nonetheless simplified and superficial -- if you're curious, we recommend that you look into the resources referred to at the bottom of this notebook. Here we follow the introductory part of [2].\n",
    "\n",
    "The **history** is the sequence of observations, actions and rewards\n",
    "\n",
    "$H_t = O_1, R_1, A_1, \\ldots, A_{t-1}, O_t, R_t$,\n",
    "\n",
    "that is, the sensorimotor stream available to e.g. an animal or a robot. What happens next is determined by the agent using the **state**. Formally, it's a function of the history:\n",
    "\n",
    "$S_t = f(H_t)$.\n",
    "\n",
    "A state $S_t$ is **Markov** is and only if\n",
    "\n",
    "$\\mathbb{P}[S_{t+1} | S_t] = \\mathbb{P}[S_{t+1} | S_1, \\ldots, S_t]$,\n",
    "\n",
    "that is, the future is independent of the past given the present. Once this state is known, the history can be discarded. Most reinforcement learning algorithms assume the Markov property, which means that the state space must be designed properly.\n",
    "\n",
    "Now, a reinforcement learning agent consists of some of these parts:\n",
    "- Policy: the agent's behavior function.\n",
    "- Value function: the agent's idea of how good a state or action is.\n",
    "- Model: the agent's representation of the environment.\n",
    "\n",
    "In general, a policy simply selects an action as a function of the current state, i.e. $a = \\pi(s)$. A value function is a prediction of cumulative future reward, which is exactly what we want the agent to maximize -- so we can also derive a policy from the value function. The model predicts what the environment will do next, and is used in model-based reinforcement learning, which we won't get into in these notebooks. Hence, we only look at model-free reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration and exploitation\n",
    "Another central concept in reinforcement learning is the tradeoff between exploration and exploitation. Since reinforcement learning is about trial-and-error, the agent needs to discover a good policy through interaction with the environment without losing too much reward along the way. Hence, the agent can explore to discover more information about the environment, or it can exploit its current knowledge to maximize the reward it currently thinks it will achieve. Both are important in order to achieve good performance, and many successful algorithms are designed with this in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cliff World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize': (15, 15)})\n",
    "states_colors = ListedColormap(\n",
    "    ['#9A9A9A', '#D886BA', '#4D314A', '#6E9183'])\n",
    "cmap_default = 'Blues'\n",
    "cpal_default = sns.color_palette((\"Blues_d\"))\n",
    "\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "env = CliffworldEnv()\n",
    "env.render(mode='reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextAreaSubmit(submit_func=env.build_world,\n",
    "              default_text = 12*'N'+'\\n'+12*'N'+'\\n'+12*'N'+'\\n'+'S'+10*'C'+'G')\n",
    "SliderParameters(env.set_rewards,{'name': 'neutral', 'min': -10, 'max': 0},{'name': 'cliff', 'min': -5000, 'max': 0},\n",
    "                 {'name': 'goal', 'min': -10, 'max': 0},{'name': 'start', 'min': -10, 'max': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(mode='reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RandomAgent()\n",
    "sum_r, ss = run_episode_ss(env, agent)\n",
    "\n",
    "env.render(mode='path', ss=ss)\n",
    "print(\"Reward achieved: \", sum_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env, agent, epsilon_decay, n_episodes) -> list:\n",
    "    rewards = []\n",
    "    for i in tqdm(range(n_episodes)):\n",
    "        sum_r = run_episode(env, agent, learn=True, max_length=5000)\n",
    "        rewards.append(sum_r)\n",
    "        agent.epsilon *= epsilon_decay\n",
    "    agent.epsilon = 0\n",
    "    sum_r, ss = run_episode_ss(env, agent)\n",
    "    print('Trained for ', n_episodes, ' episodes. Last episode achieved a reward of ', sum_r, '. Last episode run: ')     \n",
    "    env.render(mode='path', ss=ss)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "alpha = 0.1  # learning rate\n",
    "epsilon = 1.0  # initial randomness\n",
    "gamma = 1.0  # discount factor\n",
    "agent = TabularQAgent(alpha, epsilon, gamma)\n",
    "\n",
    "rewards = run_experiment(env, agent, 0.999, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = sns.tsplot(rewards)\n",
    "figure.set(xlabel='# episodes', ylabel='reward')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(mode='policy', Q=agent.Q, A=agent.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and further reading\n",
    "\n",
    "[1] [Reinforcement Learning: An Introduction, 2nd edition, by Richard S. Sutton and Andrew G. Barto. Free draft.](http://incompleteideas.net/book/the-book-2nd.html)\n",
    "\n",
    "[2] [David Silver's course on reinforcement learning (with videos).](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
