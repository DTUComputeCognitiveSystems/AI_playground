{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run global setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"../global_setup.py\") as setupfile:\n",
    "        exec(setupfile.read())\n",
    "except FileNotFoundError:\n",
    "    print('Setup already completed')\n",
    "import sys\n",
    "from os import getcwd\n",
    "sys.path.append(getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run local setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym import logger\n",
    "\n",
    "logger.set_level(logger.ERROR)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.rl.RandomAgent import RandomAgent\n",
    "from src.rl.util import run_episode\n",
    "from src.rl.TabularQAgent import TabularQAgent\n",
    "from src.rl.NeuralQAgent import NeuralQAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cart-pole environment\n",
    "\n",
    "In this notebook we're going to look at a slightly more difficult setting. Here, we have a pole balancing on top of a cart. \n",
    "The cart can move left or right along a track, and the goal is to keep the pole balanced on top, without the cart moving too far away from its starting position.  \n",
    "\n",
    "We can render the setting like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In model-free reinforcement learning, which is currently the most common and successful approach,  we do not use any prior knowledge about the system dynamics. It is worth noting that for an environment like the cart-pole, we can easily describe the dynamics using two differential equations describing the angular acceleration of the pole $\\ddot\\theta$, and the acceleration of the cart $\\ddot x$ (here we ignore friction):\n",
    "\n",
    "$T = \\frac{F + m_p L\\dot\\theta^2\\sin\\theta)}{m_p + m_c}$\n",
    "\n",
    "$\\ddot\\theta = \\frac{g \\sin\\theta - T\\cos\\theta}{L (\\frac{4}{3} - \\frac{m_p\\cos^2\\theta}{m_p + m_c})}$ \n",
    "\n",
    "$\\ddot x = T - \\frac{m_p L\\ddot\\theta\\cos\\theta}{m_p + m_c}$\n",
    "\n",
    "Don't worry, you don't need to understand these. However, with this valuable knowledge, we could manually design a control system behaving as we would like it to. However, in model-free reinforcement learning we ignore any such knowledge, letting the agent learn how to control the system from an essentially blank slate.\n",
    "\n",
    "Now let us consider how the state space for this task could be designed. Obviously we're going to need the position of the cart and the angle of the pole to be part of the state space, since these are the things we're wanting to control. Additionally, due to assuming what's called the Markov property, we'll need to include the velocity of the cart and the pole. The Markov property essentially means that we have no \"memory\" -- we need to be able to describe the evolution of the system using only the current state. In this case, we need the velocities in order to know the positions at the next time step, just like in the equations of motion.\n",
    "\n",
    "The result is a 4-dimensional state space -- the grid worlds from the previous exercise had a 2-dimensional state space. However, there is more to the picture than the dimensionality of the space. Whereas the grid worlds we looked at previous had around $4*12 = 48$ distinct states, each dimension in the 4-dimensional state space is in theory a real number. This leads to an effectively infinite number of distinct states that the pole and cart could be in -- even the smallest change in angle or position is distinct. We could even have an infinite number of actions, that is the force applied to the cart. However, for this reinforcement learning environment, we discretize the force into 2 distinct actions: a fixed force applied to either direction.\n",
    "\n",
    "First, let's see the performance of a random policy plotted as a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "agent = RandomAgent()\n",
    "run_episode(env, agent, render=True)\n",
    "dist = [run_episode(env, agent) for _ in range(1000)]\n",
    "ax = sns.distplot(dist, kde=False)\n",
    "ax.set(xlabel='Reward', ylabel='# occurences');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as an experiment, let's see how our tabular Q-learning agent that we used to solve the grid world tasks works on this environment. First, a learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = TabularQAgent(0.1, 0.5, 0.99)\n",
    "\n",
    "def run_experiment(env, agent, epsilon_decay, n_episodes) -> list:\n",
    "    rewards = []\n",
    "    for i in tqdm(range(n_episodes)):\n",
    "        sum_r = run_episode(env, agent, learn=True)\n",
    "        rewards.append(sum_r)\n",
    "        agent.epsilon *= epsilon_decay\n",
    "    agent.epsilon = 0\n",
    "    sum_r = run_episode(env, agent)\n",
    "    print('Trained for ', n_episodes, ' episodes. Last episode achieved a reward of ', sum_r)     \n",
    "    #env.render(mode='path', ss=ss)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "#run_episode(env, agent, learn=True)\n",
    "rewards = run_experiment(env, agent, 0.99, 10000)\n",
    "ax = sns.tsplot(rewards);\n",
    "sns.despine()\n",
    "ax.set(xlabel='Episode', ylabel='Reward');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the corresponding histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(rewards, kde=False)\n",
    "ax.set(xlabel='Reward', ylabel='# occurences');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It didn't seem to learn anything -- and the reason is the size of the state space. Intuitively, since we have so many states, we're unlikely to ever end up in the same state twice -- and so we'll never learn anything by storing our knowledge in a lookup table. Additionally, we should consider that what we've learned in one state might be useful in similar states. What we can do instead, is to use *function approximation*. Instead of a lookup table, we're going to learn a much simpler, continuous function that maps a state to the value we'd otherwise store in the table. Since this function is much simpler, there is no way it's going to return the exact same values as our table would -- but we can hope that it's going to be good enough.\n",
    "\n",
    "We'll use a small neural network, taking in the 4-dimensional state. Through training, it should learn to output the value of taking our two actions, applying a force to the left or right. We can then use this instead of our lookup table, choosing the action with the highest value.\n",
    "\n",
    "Training this agent is going to be significantly slower than what we've seen previously. The reason is that, as the agent gets better, each episode takes significantly longer to end -- and because the problem is harder, we're going to need more training data, and more time to make use of that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = NeuralQAgent(4, env.action_space.n, alpha=1e-3, gamma=0.95, epsilon=1.0, decay=1e-6)\n",
    "rewards = run_experiment(env, agent, 0.995, 600)\n",
    "ax = sns.tsplot(rewards)\n",
    "sns.despine()\n",
    "ax.set(xlabel='Episode', ylabel='Reward');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That learning curve should look significantly better than the tabular one. It might be unstable and jump up and down, but at least it is learning. Let's look at a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = [run_episode(env, agent) for _ in tqdm(range(200))]\n",
    "sns.distplot(dist, kde=False)\n",
    "print(\"Mean neural agent reward: \", np.mean(dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly better, and if you weren't too unlucky with the training, it should even hit the maximum reward in this environment of 500. However, the imperfect results goes to show how difficult the reinforcement learning problem is. We can even render the policy in action -- try running it a few times and see how it behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "run_episode(env, agent, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
