{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run global setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"../global_setup.py\") as setupfile:\n",
    "        exec(setupfile.read())\n",
    "except FileNotFoundError:\n",
    "    print('Setup already completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run local setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym import logger\n",
    "\n",
    "logger.set_level(logger.ERROR)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.rl.RandomAgent import RandomAgent\n",
    "from src.rl.util import run_episode\n",
    "from src.rl.TabularQAgent import TabularQAgent\n",
    "from src.rl.NeuralQAgent import NeuralQAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole Challenge\n",
    "\n",
    "In this notebook we're going to look at a more difficult setting. Here, we have a pole balancing on top of a cart. \n",
    "The cart can move left or right, and the goal is to keep the pole balanced on top, without moving too far away from the starting position.  \n",
    "\n",
    "We can render the setting like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider how the state space for this task could be designed. Obviously we're going to need the position of the cart and the angle of the pole to be part of the state space, since these are the things we're wanting to control. Additionally, due to assuming what's called the Markov property, we'll need to include the velocity of the cart and the pole. It might not be clear why we need to include the velocities, but for now you should just accept that this is the case. We're not going to discuss what the Markov property means in this course.\n",
    "\n",
    "The result is a 4-dimensional state space -- the grid world from the previous exercise had a 2-dimensional state space. However, there is more to the picture than the dimensionality of the space. Whereas the grid world had around $4*12 = 48$ distinct states, each dimension in the 4-dimensional state space is in theory a real number. This leads to an effectively infinite number of distinct states that the pole and cart could be in -- even the smallest change in angle or position is distinct.\n",
    "\n",
    "First, let's see the performance of a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "agent = RandomAgent()\n",
    "run_episode(env, agent, render=True)\n",
    "dist = [run_episode(env, agent) for _ in range(1000)]\n",
    "sns.distplot(dist, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how our tabular Q-learning agent that we used to solve the grid world task works. First, a learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = TabularQAgent(0.1, 0.5, 0.99)\n",
    "\n",
    "def run_experiment(env, agent, epsilon_decay, n_episodes) -> list:\n",
    "    rewards = []\n",
    "    for i in tqdm(range(n_episodes)):\n",
    "        sum_r = run_episode(env, agent, learn=True)\n",
    "        rewards.append(sum_r)\n",
    "        agent.epsilon *= epsilon_decay\n",
    "    agent.epsilon = 0\n",
    "    sum_r = run_episode(env, agent)\n",
    "    print('Trained for ', n_episodes, ' episodes. Last episode achieved a reward of ', sum_r)     \n",
    "    #env.render(mode='path', ss=ss)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "#run_episode(env, agent, learn=True)\n",
    "rewards = run_experiment(env, agent, 0.99, 1000)\n",
    "sns.tsplot(rewards)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the corresponding histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(rewards, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It didn't seem to learn anything -- and the reason is the size of the state space. Intuitively, since we have so many states, we're unlikely to ever end up in the same state twice -- and so we'll never learn anything by storing our knowledge in a lookup table. What we can do instead, is to use *function approximation*. Instead of a lookup table, we're going to learn a much simpler, continuous function that maps a state to the value we'd otherwise store in the table. Since this function is much simpler, there is no way it's going to return the exact same values as our table would -- but we can hope that it's going to be good enough.\n",
    "\n",
    "Training this agent is going to be significantly slower than what we've seen previously. The reason is that, as the agent gets better, each episode takes significantly longer to end -- and because the problem is harder, we're going to need more training data, and more time to make use of that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = NeuralQAgent(4, env.action_space.n, alpha=0.001, gamma=0.95, epsilon=1.0)\n",
    "rewards = run_experiment(env, agent, 0.995, 500)\n",
    "sns.tsplot(rewards)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That learning curve should look significantly better than the tabular one. It might be unstable and jump up and down, but at least it is learning. Let's look at a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = [run_episode(env, agent) for _ in tqdm(range(200))]\n",
    "sns.distplot(dist, kde=False)\n",
    "print(\"Mean neural agent reward: \", np.mean(dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significantly better, often reaching the maximal reward of 500 in this environment. We can even render the policy in action -- try running it a few times and see how it behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "run_episode(env, agent, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
