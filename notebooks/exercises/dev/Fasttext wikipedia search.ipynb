{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Fasttext word embeddings to search Danish Wikipedia\n",
    "This notebook goes step-by-step through the following:\n",
    "1. Load the Fasttext model for Danish language\n",
    "2. Write a function to compute the vectorized representations of text in Danish\n",
    "3. Compute vectorized representations for all the abstracts in Danish wikipedia\n",
    "4. Write a function to compute the cosine distance between the vectorized representations\n",
    "5. Test everything on some news headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"../../global_setup.py\") as setupfile:\n",
    "        exec(setupfile.read())\n",
    "except FileNotFoundError:\n",
    "    print('Setup already completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from gensim.test.utils import common_texts\n",
    "from scipy.spatial.distance import cdist\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Fasttext model for Danish language\n",
    "1. In the root folder of the project, create a directory \"data\" and a subdirectory \"fasttext\".\n",
    "2. Download the pretrained danish fasttext model from here: \n",
    "   https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.da.zip\n",
    "3. Place the files **wiki.da.bin** and **wiki.da.vec** in the folder you have created in (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_path = Path(\"data\", \"fasttext\", \"wiki.da.bin\")\n",
    "model = FastText.load_fasttext_format(str(bin_path))\n",
    "#print(model.most_similar('æble'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "1. Compute vectorized representations of all danish wikipedia abstracts\n",
    "2. Compute vectorized representations of news titles\n",
    "3. Find wikipedia article with minimal cosine distance from a given news title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Working with Danish wikipedia abstracts\n",
    "Here we compute the vectorized representations of the Danish wikipedia abstracts using the pretrained Fasttext model that we just loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step: load the Danish wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parsed documents.\n",
      "Loading preprocessed documents.\n",
      "Wikipedia loaded.\n"
     ]
    }
   ],
   "source": [
    "from src.text.document_retrieval.wikipedia import Wikipedia\n",
    "wikipedia = Wikipedia(\n",
    "    language=\"Danish\",\n",
    "    cache_directory_url=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second step: write a function to calculate the vectorized representation of any text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumVectorRepresentation(text, verbose = False):\n",
    "    pattern = re.compile('[^a-zA-Z0-9åÅøØæÆ ]+', re.UNICODE)\n",
    "    text = pattern.sub('', text)\n",
    "    words = text.lower().strip().split()\n",
    "    text_vector = np.zeros(model.wv[\"a\"].shape)\n",
    "    if verbose:\n",
    "        print(\"len: {}, words: {}\".format(len(words), words))\n",
    "    for i in range(len(words)):\n",
    "        try:\n",
    "            text_vector = text_vector + model.wv[words[i]]\n",
    "        except KeyError as e:\n",
    "            if verbose:\n",
    "                print(\"i: {}, e: {}\".format(i, e))\n",
    "            continue\n",
    "    return text_vector\n",
    "    \n",
    "#sumVectorRepresentation(\"Han sagde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third step: compute vectorized representations for all Danish wikipedia abstracts\n",
    "* All empty abstracts and those that do not have any alphanumeric symbols are removed and not considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "i_max = 0\n",
    "n_removed = 0\n",
    "wikipedia.documents_clean = wikipedia.documents.copy()\n",
    "wikipedia_abstract_vectors = []\n",
    "wikipedia_title_vectors = []\n",
    "pattern1 = re.compile('[^a-zA-Z0-9åÅøØæÆ]+', re.UNICODE)\n",
    "\n",
    "for n in range(len(wikipedia.documents)):\n",
    "    # if abstract length is zero, remove it\n",
    "    try:\n",
    "        if len(pattern1.sub('', wikipedia.documents[n].abstract)) == 0:\n",
    "            del wikipedia.documents_clean[n - n_removed]\n",
    "            n_removed = n_removed + 1\n",
    "        else:\n",
    "            wikipedia_abstract_vectors.append(sumVectorRepresentation(wikipedia.documents[n].abstract))\n",
    "            wikipedia_title_vectors.append(sumVectorRepresentation(wikipedia.documents[n].title))\n",
    "            i = i + 1\n",
    "            if i_max > 0 and i > i_max:\n",
    "                break\n",
    "    except IndexError as e:\n",
    "        print(\"n: {}, n_removed: {}, w.d: {}, w.d_c: {}\".format(n, n_removed, len(wikipedia.documents), len(wikipedia.documents_clean)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate the cosine distance between the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdist_func(A, B):\n",
    "    dists = cdist(A, B, 'cosine')\n",
    "    return np.argmin(dists, axis=0), dists #np.min(dists, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test the results on some news headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable below defines the number of relevant wikipedia articles to consider\n",
    "n_wiki_matches = 3\n",
    "# Variable below is an example news headline\n",
    "example_title = \"Tyske myndigheder undersøger 95.000 biler af mærket Opel\"\n",
    "# Hawaii: Flyv med helikopter hen over Kauai — en af verdens smukkeste øer\n",
    "# Tyske myndigheder undersøger 95.000 biler af mærket Opel\n",
    "# Salmonella fundet i kalkunbryst solgt i Aldi-butikker\n",
    "\n",
    "\n",
    "# Calculate the vectorized representation\n",
    "example_title_vector = sumVectorRepresentation(example_title)\n",
    "\n",
    "cdist_result = cdist_func(wikipedia_abstract_vectors, [example_title_vector])\n",
    "cdist_list = cdist_result[1] # List of all the cosine distances\n",
    "cdist_list_sorted = np.sort(cdist_list, axis = 0) # Sorted list of cosine distances - to get top N matches\n",
    "\n",
    "# Print the results\n",
    "print(\"Example headline: {}\\r\\n\".format(example_title))\n",
    "## Print all the matches with their abstracts\n",
    "for i in range(n_wiki_matches):\n",
    "    result = np.where(cdist_list == cdist_list_sorted[i])\n",
    "    print(\"{} Wikipedia article {}: \\r\\n Abstract: {}\\r\\n\".format(i, \n",
    "                                                       wikipedia.documents_clean[result[0][0]],\n",
    "                                                       wikipedia.documents_clean[result[0][0]].abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from ipywidgets.widgets import Accordion, HTML\n",
    "from notebooks.exercises.src.text.news_sentiment_1 import RSSDashboard\n",
    "RSSdb = RSSDashboard()\n",
    "s = RSSdb._do_sentiment_analysis(selected_value = 0)\n",
    "\n",
    "list_labels = []\n",
    "for i in range(len(RSSdb.data_titles)):\n",
    "    result_content = \"<ol>\"\n",
    "    cdist_result = cdist_func(wikipedia_abstract_vectors, [sumVectorRepresentation(RSSdb.data_titles[i])])\n",
    "    cdist_list = cdist_result[1] # List of all the cosine distances\n",
    "    cdist_list_sorted = np.sort(cdist_list, axis = 0) # Sorted list of cosine distances - to get top N matches\n",
    "    \n",
    "    \n",
    "    ## Print all the matches with their abstracts\n",
    "    for i in range(n_wiki_matches):\n",
    "        result = np.where(cdist_list == cdist_list_sorted[i])\n",
    "        result_content = result_content + \"<li>{}: <p> {}</p>\".format(wikipedia.documents_clean[result[0][0]].title,\n",
    "                                                                          wikipedia.documents_clean[result[0][0]].abstract)\n",
    "    result_content = result_content + \"<ol>\"\n",
    "    list_labels.append(HTML(value = result_content))\n",
    "\n",
    "accordion = Accordion(children = (list_labels),)\n",
    "\n",
    "for i in range(len(RSSdb.data_titles)):\n",
    "    accordion.set_title(i, \"{}. {}\".format(i + 1, RSSdb.data_titles[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(accordion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.text import Text\n",
    "#polyglot download embeddings2.da\n",
    "#polyglot download ner2.da\n",
    "\n",
    "blob = \"Finanstilsynet afviser Danske Banks kronprins som direktør\"\n",
    "text = Text(blob, hint_language_code='da')\n",
    "text.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(RSSdb.data_titles)):\n",
    "    text = Text(RSSdb.data_titles[i], hint_language_code='da')\n",
    "    print(\"{} \\r\\n {} \\r\\n\".format(RSSdb.data_titles[i], text.entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polyglot download pos2.da\n",
    "for i in range(len(RSSdb.data_titles)):\n",
    "    text = Text(RSSdb.data_titles[i], hint_language_code='da')\n",
    "    print(\"\\r\\n\\r\\n {} \\r\\n\".format(RSSdb.data_titles[i]))\n",
    "    print(\"{:<16}{}\".format(\"Word\", \"POS Tag\")+\"\\n\"+\"-\"*30)\n",
    "    for word, tag in text.pos_tags:\n",
    "        print(u\"{:<16}{:>2}\".format(word, tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK was unable to find the TreeTagger bin!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TreeTagger' object has no attribute '_treetagger_bin'",
     "output_type": "error",
     "traceback": [
      "---------------------------------------------------------------------------",
      "AttributeError                            Traceback (most recent call last)",
      "<ipython-input-4-63a2d0dc0557> in <module>()\n      1 from treetagger import TreeTagger\n      2 tt = TreeTagger(path_to_treetagger='C:/TreeTagger/', language = \"danish\")\n----> 3 tt.tag('Dette er en sætning.')\n      4 tt.get_installed_lang()\n      5 treetagger.__file__\n",
      "~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\treetagger.py in tag(self, sentences)\n    156         # Run the tagger and get the output\n    157         if(self._abbr_list is None):\n--> 158             p = Popen([self._treetagger_bin],\n    159                         shell=False, stdin=PIPE, stdout=PIPE, stderr=PIPE)\n    160         elif(self._abbr_list is not None):\n",
      "AttributeError: 'TreeTagger' object has no attribute '_treetagger_bin'"
     ]
    }
   ],
   "source": [
    "from treetagger import TreeTagger\n",
    "tt = TreeTagger(path_to_treetagger='C:/TreeTagger/', language = \"danish\")\n",
    "tt.tag('Dette er en sætning.')\n",
    "tt.get_installed_lang()\n",
    "treetagger.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'TREETAGGER_HOME' in os.environ\n",
    "os.environ['TREETAGGER_HOME'] = \"C:\\TreeTagger\"\n",
    "'TREETAGGER_HOME' in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = \"Søren Hansen om den »nøgne sandhed«: Erdogan går på diplomatiske listefødder for at forbedre sit forhold til USA\"\n",
    "#s = \"Google fyrer 13 chefer og 35 medarbejdere for sexchikane\"\n",
    "s = \"Søren Hansen hader Adolf Hitler\"\n",
    "pattern = re.compile('[^a-zA-Z0-9åÅøØæÆ ]+', re.UNICODE)\n",
    "s = pattern.sub('', s)\n",
    "words = s.lower().strip().split()\n",
    "words_copy = words.copy()\n",
    "n_removed = 0\n",
    "stop_words = [\"den\", \"det\", \"en\", \"et\", \"om\", \"for\", \"og\", \"til\", \"at\", \"på\", \"som\", \"jeg\", \"mig\", \"mine\", \"min\", \"mit\", \"du\", \"dig\", \"din\", \"dit\", \"dine\", \"han\", \"ham\", \"hun\", \"hende\", \"de\", \"dem\", \"vi\", \"os\", \"sin\", \"sit\", \"sine\", \"sig\"]\n",
    "for i in range(len(words)):\n",
    "    if words[i] in stop_words:\n",
    "        words_copy.pop(i - n_removed)\n",
    "        n_removed = n_removed + 1\n",
    "\n",
    "for i in range(len(words_copy)):\n",
    "    if i > 0:\n",
    "        words_copy.append(words_copy[i - 1] + \" \" + words_copy[i])\n",
    "\n",
    "words = words_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = []\n",
    "for i in range(len(words)):\n",
    "    cdist_result = cdist_func(wikipedia_title_vectors, [sumVectorRepresentation(words[i])])\n",
    "    cdist_result2 = cdist_func([sumVectorRepresentation(s)], [sumVectorRepresentation(words[i])])\n",
    "    \n",
    "    cdist_list1 = cdist_result[1] # List of all the cosine distances\n",
    "    cdist_list2 = cdist_result2[1]\n",
    "    cdist_list = (cdist_list1 + cdist_list2) / 2\n",
    "    cdist_list_sorted = np.sort(cdist_list, axis = 0) # Sorted list of cosine distances - to get top N matches\n",
    "    \n",
    "    x = np.where(cdist_list == cdist_list_sorted[0])[0]\n",
    "    r.append( (x, cdist_list[x][0]))\n",
    "    #print(\"{} {} {} {}\".format(x, wikipedia.documents_clean[x[0]].title, cdist_list[x], words[i]))\n",
    "\n",
    "# When np.where returns multiple matches, we flatten them\n",
    "r_copy = r.copy()\n",
    "uniques = []\n",
    "for i in range(len(r)-1, -1, -1):\n",
    "    if len(r[i][0]) > 1:\n",
    "        r_copy.pop(i)\n",
    "        for j in range(len(r[i][0])):\n",
    "            r_copy.append( (np.array([r[i][0][j]]), r[i][1]))\n",
    "\n",
    "# Remove duplicate wikipedia pages. They occur because different n-grams can match the same pages\n",
    "for i in range(len(r_copy)-1,-1,-1):\n",
    "    if r_copy[i][0] in uniques:\n",
    "        r_copy.pop(i)\n",
    "    else:\n",
    "        uniques.append(r_copy[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = r_copy\n",
    "# Transform into list of tuples\n",
    "r = [ (r[i][0][0], r[i][1][0]) for i in range(len(r))]\n",
    "# Sort the list of tuples by cosine distance\n",
    "r = sorted(r, key=lambda tup: tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(r)):\n",
    "    print(\"{:.2f} {}: \\r\\n Abstract: {}\\r\\n\".format(r[i][1], \n",
    "                                                       wikipedia.documents_clean[r[i][0]].title,\n",
    "                                                       wikipedia.documents_clean[r[i][0]].abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
