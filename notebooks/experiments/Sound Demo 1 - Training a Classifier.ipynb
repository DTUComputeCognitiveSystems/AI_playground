{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sound Demo 1\n",
    "This demo illustrates how to train a binary and a multilabel sound classifier using your microphone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run global setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"../global_setup.py\") as setupfile:\n",
    "        exec(setupfile.read())\n",
    "except FileNotFoundError:\n",
    "    print('Setup already completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run local setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.experiments.src.sound_demos.live_predictions import LivePredictions, run_livepred\n",
    "from notebooks.experiments.src.sound_demos.multilabel_classifier import Recorder, SoundClassifier\n",
    "from src.audio.mini_recorder import miniRecorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classifier\n",
    "We are going to use a pretrained model to construct a binary sound classifier. The model is a deep convolutional nerual network in the same style as the [VGG16 network](https://arxiv.org/pdf/1409.1556.pdf), with slightly different settings and fewer layers. The VGG16 network was originally used for images so how can we use it for sound? Instead of working directly with the waveform signal we can work with the spectrogram, a 2d image, instead.  \n",
    "The pretraining was done on the [UrbanSound8K dataset](https://urbansounddataset.weebly.com/urbansound8k.html).  This contains more than 8000 examples up to 4 seconds long of sound from 10 different classes (children playing, dogs barking among others). Our hope is that the network has learnt some general audio-features from the spetrogram, that we can use to distinguish between two classes of sound of your choice.  \n",
    "\n",
    "### Create dataset\n",
    "First we create or own dataset as basis for training. The below cell will start a recording process where you first record $n$ examples of the first class, followed by $n$ examples of the second class. Whenever a recording is finished the next one starts immediately after.  \n",
    "\n",
    "Things to keep in mind\n",
    "- You can decide to just make the sound of class 0 throughout the recording time for class 0, or you can try to match exactly one example of this sound for each recording. Whatever you choose, make sure to do the same for the second class. What do you think will happen if there is a lot of silence in class 0 recordings, but not in class 1 recordings?\n",
    "- How many examples do we need in each class in order to get a good classifier?\n",
    "- How will background noise affect performance?\n",
    "- What happens (should happen) at test-time if sounds from both classes are present in a recording?\n",
    "\n",
    "The recorded files will be located in the folder you specify as `wav_dir`. E.g if you wish to record 10 files for each class and locate them in `/Users/me/sound` on you computer, the first line of code should look like this\n",
    "```python\n",
    "recorder = Recorder(n_classes=2, n_files = 10, prefix='binary', wav_dir='/Users/me/sound')\n",
    "```\n",
    "By default, the files will be saved in the tmp directory in the root folder of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder = Recorder(n_classes=2, n_files = 12, prefix='binary', wav_dir='tmp')\n",
    "recorder.record(seconds=2)\n",
    "data = recorder.create_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you already recorded a dataset you can reload it using this piece of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder = Recorder(n_classes=2, n_files = 12, prefix='binary', wav_dir='tmp')\n",
    "data = recorder.create_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the binary classifier\n",
    "Run the cell below to train the binary classifier using the pre-trained weights.\n",
    "<!--You need to download the pre-trained weights for the neural network from [here](https://drive.google.com/file/d/1BXe5KZcZVqFzBMJo6FZ78hQ8j2CCRr3X/view?usp=sharing).  \n",
    "Place the file containing the weights somewehere on your computer and then specify the full path below. E.g. if the location of the file is  ```/Users/me/sound/sound_classification_weights.hdf5 ``` the first line of code should look like this\n",
    "```python\n",
    "binary_classifier = SoundClassifier(weights_path='/Users/me/sound/sound_classification_weights.hdf5')\n",
    "```-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binary_classifier = SoundClassifier()\n",
    "binary_classifier.train(data=data)\n",
    "binary_classifier.plot_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model\n",
    "Now we have a model that is trained to discriminate between two sounds. Try to make a recording of sound from one of the classes (or something completely different) and see what it is classified as by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = miniRecorder(seconds=1.5)\n",
    "_ = rec.record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classifier.predict(sound_clip=rec.sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.playback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live Predictions\n",
    "Let us use the live spectrogram to visualize the sound input to the microphone continuously and get running predictions from the model.\n",
    "What happens?\n",
    "- Does your binary classifier work?\n",
    "- Does the model predict one of the classes even when there is silence / background noise? Why? Do you have any ideas how to mitigate this?\n",
    "- What happens if you produce sound from both classes at the same time? What should ideally happen? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_livepred(predictor=binary_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
