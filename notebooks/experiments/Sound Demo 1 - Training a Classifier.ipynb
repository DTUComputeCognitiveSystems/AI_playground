{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sound Demo 1\n",
    "This demo illustrates how to train a binary and a multilabel sound classifier using your microphone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run global setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"../global_setup.py\") as setupfile:\n",
    "        exec(setupfile.read())\n",
    "except FileNotFoundError:\n",
    "    print('Setup already completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run local setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.experiments.src.sound_demos.live_predictions import LivePredictions, run_livepred\n",
    "from notebooks.experiments.src.sound_demos.multilabel_classifier import Recorder, SoundClassifier\n",
    "from src.audio.mini_recorder import miniRecorder\n",
    "from notebooks.experiments.src.sound_demos.sound_demo_1_db import SoundDemo1Dashboard1, SoundDemo1Dashboard2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classifier\n",
    "We are going to use a pretrained model to construct a binary sound classifier. The model is a deep convolutional nerual network in the same style as the [VGG16 network](https://arxiv.org/pdf/1409.1556.pdf), with slightly different settings and fewer layers. The VGG16 network was originally used for images so how can we use it for sound? Instead of working directly with the waveform signal we can work with the spectrogram, a 2d image, instead.  \n",
    "The pretraining was done on the [UrbanSound8K dataset](https://urbansounddataset.weebly.com/urbansound8k.html).  This contains more than 8000 examples up to 4 seconds long of sound from 10 different classes (children playing, dogs barking among others). Our hope is that the network has learnt some general audio-features from the spetrogram, that we can use to distinguish between two classes of sound of your choice.  \n",
    "\n",
    "### Create dataset\n",
    "First we create or own dataset as basis for training. The below cell will start a recording process where you first record $n$ examples of the first class, followed by $n$ examples of the second class. Whenever a recording is finished the next one starts immediately after.  \n",
    "\n",
    "Things to keep in mind\n",
    "- You can decide to just make the sound of class 0 throughout the recording time for class 0, or you can try to match exactly one example of this sound for each recording. Whatever you choose, make sure to do the same for the second class. What do you think will happen if there is a lot of silence in class 0 recordings, but not in class 1 recordings?\n",
    "- How many examples do we need in each class in order to get a good classifier?\n",
    "- How will background noise affect performance?\n",
    "- What happens (should happen) at test-time if sounds from both classes are present in a recording?\n",
    "\n",
    "The recorded files will be located in the folder you specify as `wav_dir`. E.g if you wish to record 10 files for each class and locate them in `/Users/me/sound` on you computer, the first line of code should look like this\n",
    "```python\n",
    "recorder = Recorder(n_classes=2, n_files = 10, prefix='binary', wav_dir='/Users/me/sound')\n",
    "```\n",
    "By default, the files will be saved in the tmp directory in the root folder of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78feb464a65409b984c24530771710d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>VBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Dropdown(description='Choose training data:', layout=Layout(width='400px'), options={'Record training data': 0, 'Load file ...': 1}, style=DescriptionStyle(description_width='160px'), value=0), Dropdown(description='Choose recording length:', layout=Layout(display='block', width='400px'), options={'2s': 2, '3s': 3, '5s': 5}, style=DescriptionStyle(description_width='160px'), value=2), Dropdown(description='Choose number of classes:', layout=Layout(width='400px'), options={'2': 2, '3': 3, '4': 4}, style=DescriptionStyle(description_width='160px'), value=2), Dropdown(description='Choose number of files:', layout=Layout(width='400px'), options={'12': 12, '8': 8, '6': 6, '4': 4, '2': 2}, style=DescriptionStyle(description_width='160px'), value=12))), VBox(children=(Button(button_style='success', description='Get data', style=ButtonStyle(), tooltip='TRAIN: use recorded sounds or record new'), Button(button_style='info', description='Play the recording', style=ButtonStyle(), tooltip='TRAIN: play recorded or loaded sounds'))))), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sdb1 = SoundDemo1Dashboard1()\n",
    "sdb1.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         3.0517578e-05,  3.0517578e-05,  0.0000000e+00],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "        -7.3242188e-04, -7.6293945e-04, -7.3242188e-04],\n",
      "       [ 1.3427734e-03,  4.5776367e-04, -9.4604492e-04, ...,\n",
      "        -4.5776367e-04, -5.1879883e-04, -5.7983398e-04],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "        -7.9345703e-04,  8.8500977e-04,  1.7700195e-03]], dtype=float32), array([[1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.]]))\n"
     ]
    }
   ],
   "source": [
    "print(sdb1.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder = Recorder(n_classes=2, n_files = 12, prefix='binary', wav_dir='tmp')\n",
    "recorder.record(seconds=2)\n",
    "data = recorder.create_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you already recorded a dataset you can reload it using this piece of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'ing0'",
     "output_type": "error",
     "traceback": [
      "---------------------------------------------------------------------------",
      "ValueError                                Traceback (most recent call last)",
      "<ipython-input-11-ca4ef17a57f3> in <module>()\n      1 recorder = Recorder(n_classes=2, n_files = 2, prefix='train', wav_dir='tmp')\n----> 2 data = recorder.create_dataset()\n",
      "~/Documents/GitHub/AI_playground/notebooks/experiments/src/sound_demos/multilabel_classifier.py in create_dataset(self)\n    129         data = np.array(data)\n    130 \n--> 131         labels = np.array(labels, dtype=np.int)        \n    132         n_labels = len(labels)\n    133         one_hot_encode = np.zeros((n_labels, self.n_classes))\n",
      "ValueError: invalid literal for int() with base 10: 'ing0'"
     ]
    }
   ],
   "source": [
    "recorder = Recorder(n_classes=2, n_files = 2, prefix='train', wav_dir='tmp')\n",
    "data = recorder.create_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the binary classifier\n",
    "Run the cell below to train the binary classifier using the pre-trained weights.\n",
    "<!--You need to download the pre-trained weights for the neural network from [here](https://drive.google.com/file/d/1BXe5KZcZVqFzBMJo6FZ78hQ8j2CCRr3X/view?usp=sharing).  \n",
    "Place the file containing the weights somewehere on your computer and then specify the full path below. E.g. if the location of the file is  ```/Users/me/sound/sound_classification_weights.hdf5 ``` the first line of code should look like this\n",
    "```python\n",
    "binary_classifier = SoundClassifier(weights_path='/Users/me/sound/sound_classification_weights.hdf5')\n",
    "```-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "---------------------------------------------------------------------------",
      "ValueError                                Traceback (most recent call last)",
      "<ipython-input-10-20a82a679b16> in <module>()\n      1 binary_classifier = SoundClassifier()\n----> 2 binary_classifier.train(data=data)\n      3 binary_classifier.plot_training()\n",
      "~/Documents/GitHub/AI_playground/notebooks/experiments/src/sound_demos/multilabel_classifier.py in train(self, data, wav_dir)\n    194         \n    195         # Scale frequencies across sound clips\n--> 196         self._scaling()\n    197         \n    198         # mix classes\n",
      "~/Documents/GitHub/AI_playground/notebooks/experiments/src/sound_demos/multilabel_classifier.py in _scaling(self)\n    409         # for each channel, compute scaling factor\n    410         self.scaler_list = []\n--> 411         (n_clips, n_time, n_freq, n_channel) = self.features.shape\n    412 \n    413         for channel in range(n_channel):\n",
      "ValueError: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "binary_classifier = SoundClassifier()\n",
    "binary_classifier.train(data=data)\n",
    "binary_classifier.plot_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model\n",
    "Now we have a model that is trained to discriminate between two sounds. Try to make a recording of sound from one of the classes (or something completely different) and see what it is classified as by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = miniRecorder(seconds=1.5)\n",
    "_ = rec.record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classifier.predict(sound_clip=rec.sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.playback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live Predictions\n",
    "Let us use the live spectrogram to visualize the sound input to the microphone continuously and get running predictions from the model.\n",
    "What happens?\n",
    "- Does your binary classifier work?\n",
    "- Does the model predict one of the classes even when there is silence / background noise? Why? Do you have any ideas how to mitigate this?\n",
    "- What happens if you produce sound from both classes at the same time? What should ideally happen? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_livepred(predictor=binary_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
